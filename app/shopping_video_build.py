# -*- coding: utf-8 -*-
from __future__ import annotations

import datetime
import json
import re
import shutil
import os
import random
import requests
import subprocess
import time
import uuid
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
from pydub import AudioSegment

from app.utils import (
    AI,
    load_json,
    save_json,
    ensure_dir
)
from app import settings
from app.video_build import build_shots_with_i2v, concatenate_scene_clips, fill_prompt_movie_with_ai
from app.audio_sync import get_audio_duration

def _now_str() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# -----------------------------------------------------------------------------
# [New] ComfyUI ì œì¶œ ë° ëŒ€ê¸° í•¨ìˆ˜ (ë…ë¦½í˜•)
# -----------------------------------------------------------------------------
def _submit_and_wait_local(
        base_url: str,
        graph: dict,
        timeout: int = 900,
        poll: float = 2.0,
        on_progress: Optional[Callable[[Dict[str, Any]], None]] = None
) -> dict:
    """ComfyUI ì›Œí¬í”Œë¡œìš° ì œì¶œ ë° ëŒ€ê¸°"""
    client_id = str(uuid.uuid4())
    payload = {"prompt": graph, "client_id": client_id}

    try:
        resp = requests.post(f"{base_url}/prompt", json=payload, timeout=30)
        resp.raise_for_status()
        prompt_id = resp.json().get("prompt_id")
    except Exception as e:
        raise RuntimeError(f"ComfyUI ì œì¶œ ì‹¤íŒ¨: {e}")

    start_t = time.time()
    while True:
        elapsed = time.time() - start_t
        if elapsed > timeout:
            raise TimeoutError(f"ComfyUI ì‹œê°„ ì´ˆê³¼ ({elapsed:.1f}s)")

        try:
            h_resp = requests.get(f"{base_url}/history/{prompt_id}", timeout=10)
            if h_resp.status_code == 200:
                h_data = h_resp.json()
                if prompt_id in h_data:
                    return h_data[prompt_id]
        except Exception:
            pass
        time.sleep(poll)


# -----------------------------------------------------------------------------
# 1. Zonos TTS ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------------------------
def _get_zonos_config(scene: Dict[str, Any], ai: AI = None) -> Dict[str, Any]:
    """
    ì”¬ ì •ë³´ì—ì„œ Zonosìš© ì„¤ì •(emotion, speed)ì„ ê°€ì ¸ì˜¤ê±°ë‚˜,
    ê¸°ì¡´ í…ìŠ¤íŠ¸ í¬ë§·([í†¤] ë‚´ìš© [íš¨ê³¼ìŒ])ì¼ ê²½ìš° AIë¡œ ë¶„ì„í•´ ë³€í™˜(Migration)í•œë‹¤.
    """
    # 1. ì´ë¯¸ ì„¤ì •ê°’ì´ ìˆìœ¼ë©´ ë°˜í™˜
    if "voice_config" in scene:
        return scene["voice_config"]

    # 2. ì„¤ì •ê°’ì´ ì—†ìœ¼ë©´ ë‚´ë ˆì´ì…˜ íŒŒì‹± ì‹œë„
    raw_narr = scene.get("narration", "").strip()
    if not raw_narr:
        return {"speed": 1.0, "emotion": {"neutral": 1.0}}

    # ì •ê·œì‹ìœ¼ë¡œ [í†¤] ë‚´ìš© [íš¨ê³¼ìŒ] ë¶„ë¦¬
    tone_match = re.match(r"^\[(.*?)\]\s*(.*)", raw_narr)

    tone_text = "calm"
    clean_text = raw_narr
    sfx_text = ""

    if tone_match:
        tone_text = tone_match.group(1).strip()
        remain = tone_match.group(2).strip()

        # ë’¤ìª½ SFX ì²´í¬
        sfx_match = re.search(r"\s*\[(.*?)\]$", remain)
        if sfx_match:
            sfx_text = sfx_match.group(1).strip()
            clean_text = remain[:sfx_match.start()].strip()
        else:
            clean_text = remain

        # ë”°ì˜´í‘œ ì œê±°
        clean_text = clean_text.strip("'").strip('"')

    # AIì—ê²Œ ìˆ˜ì¹˜ ë³€í™˜ ìš”ì²­ (Migration)
    if ai:
        try:
            sys_p = (
                "You are a Voice Director. Analyze the 'Tone Description' and convert it into Zonos TTS parameters.\n"
                "Output JSON only: { \"speed\": float(0.8-1.5), \"emotion\": { neutral, happy, sad, disgust, fear, surprise, anger, other } (sum approx 1.0) }"
            )
            user_p = f"Tone Description: \"{tone_text}\""
            res = ai.ask_smart(sys_p, user_p, prefer="openai")

            # JSON íŒŒì‹±
            if "```" in res: res = res.split("```")[1].replace("json", "")
            config = json.loads(res[res.find("{"):res.rfind("}") + 1])

            # ì”¬ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì˜êµ¬ ì €ì¥ìš©)
            scene["narration"] = clean_text
            scene["voice_config"] = config
            if sfx_text:
                scene["sfx"] = sfx_text

            return config
        except Exception as e:
            print(f"âš ï¸ í†¤ ë¶„ì„ ì‹¤íŒ¨: {e}")

    # ì‹¤íŒ¨/AI ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    return {"speed": 1.0, "emotion": {"neutral": 1.0}}


def generate_tts_zonos(
        text: str,
        out_path: Path,
        ref_audio: Path,
        comfy_host: str = "http://127.0.0.1:8188",
        config: Dict[str, Any] = None
) -> bool:
    if not text:
        return False

    wf_path = Path(settings.JSONS_DIR) / "who_voice.json"
    if not wf_path.exists():
        wf_path = Path(r"C:\my_games\shorts_make\app\jsons\who_voice.json")

    if not wf_path.exists():
        print(f"âŒ TTS ì›Œí¬í”Œë¡œìš° ì—†ìŒ: {wf_path}")
        return False

    try:
        with open(wf_path, "r", encoding="utf-8") as f:
            graph = json.load(f)
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    if not ref_audio.exists():
        print(f"âŒ ì°¸ì¡° ì˜¤ë””ì˜¤ ì—†ìŒ: {ref_audio}")
        return False

    comfy_input_dir = Path(settings.COMFY_INPUT_DIR)
    comfy_input_dir.mkdir(parents=True, exist_ok=True)

    ref_copy_name = f"ref_{uuid.uuid4().hex[:8]}{ref_audio.suffix}"
    dst_ref = comfy_input_dir / ref_copy_name
    shutil.copy2(ref_audio, dst_ref)

    # 1. í…ìŠ¤íŠ¸/ì‹œë“œ/ì†ë„ ì„¤ì •
    found_gen = False
    for nid, node in graph.items():
        # Zonos ë…¸ë“œ ì°¾ê¸° (class_typeì— Zonos í¬í•¨ & inputsì— speechê°€ ìˆëŠ” ë…¸ë“œ)
        if "Zonos" in node.get("class_type", "") and "speech" in node.get("inputs", {}):
            node["inputs"]["speech"] = text
            node["inputs"]["seed"] = random.randint(1, 2 ** 32)
            if config and "speed" in config:
                node["inputs"]["speed"] = config["speed"]
            found_gen = True
            break

    # ë§Œì•½ ìœ„ ë£¨í”„ì—ì„œ ëª» ì°¾ì•˜ìœ¼ë©´ ID 24ë²ˆ ì‹œë„ (fallback)
    if not found_gen and "24" in graph:
        graph["24"]["inputs"]["speech"] = text
        graph["24"]["inputs"]["seed"] = random.randint(1, 2 ** 32)
        if config and "speed" in config:
            graph["24"]["inputs"]["speed"] = config["speed"]

    # 2. ê°ì • ì„¤ì • (Zonos Emotion ë…¸ë“œ)
    if config and "emotion" in config:
        emotions = config["emotion"]
        for nid, node in graph.items():
            if node.get("class_type") == "Zonos Emotion":
                for k, v in emotions.items():
                    if k in node["inputs"]:
                        node["inputs"][k] = v
                break

    # 3. ì°¸ì¡° ì˜¤ë””ì˜¤ ì„¤ì •
    found_audio = False
    for nid, node in graph.items():
        if node.get("class_type") == "LoadAudio":
            node["inputs"]["audio"] = ref_copy_name
            found_audio = True
            break

    if not found_audio and "12" in graph:
        graph["12"]["inputs"]["audio"] = ref_copy_name

    try:
        res = _submit_and_wait_local(comfy_host, graph, timeout=300)
        outputs = res.get("outputs", {})
        for nid, out_d in outputs.items():
            if "audio" in out_d:
                for item in out_d["audio"]:
                    fname = item["filename"]
                    params = {"filename": fname, "subfolder": item.get("subfolder", ""),
                              "type": item.get("type", "output")}
                    resp = requests.get(f"{comfy_host}/view", params=params)
                    if resp.status_code == 200:
                        ensure_dir(out_path.parent)
                        with open(out_path, "wb") as f:
                            f.write(resp.content)
                        return True
        return False
    except Exception as e:
        print(f"TTS ìƒì„± ì‹¤íŒ¨: {e}")
        return False



# -----------------------------------------------------------------------------
# 1.5 BGM ìƒì„± í•¨ìˆ˜ (Ace-Step) - [New]
# -----------------------------------------------------------------------------
def generate_bgm_acestep(
        prompt: str,
        out_path: Path,
        duration_sec: float,
        comfy_host: str = "http://127.0.0.1:8188",
        negative_prompt: str = "vocal, vocals, singing, human voice, lyrics, rap, speech, chant, noise, distortion"
) -> bool:
    """
    [ì‡¼í•‘ ì „ìš©] Ace-Step ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ìŒ(BGM) ìƒì„±
    - ê°€ì‚¬ ìƒì„± ë°©ì§€(lyrics_strength=0) ë° Instrumental ê°•ì œ
    """
    if not prompt:
        return False

    # ì›Œí¬í”Œë¡œìš° ë¡œë“œ
    wf_path = Path(settings.JSONS_DIR) / "ace_step_1_t2mm.json"
    if not wf_path.exists():
        # ê¸°ë³¸ ê²½ë¡œ í´ë°±
        wf_path = Path(r"C:\my_games\shorts_make\app\jsons\ace_step_1_t2mm.json")

    if not wf_path.exists():
        print(f"âŒ Ace-Step ì›Œí¬í”Œë¡œìš° ì—†ìŒ: {wf_path}")
        return False

    try:
        with open(wf_path, "r", encoding="utf-8") as f:
            graph = json.load(f)
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # ë…¸ë“œ ë§¤í•‘ ë° ê°’ ì£¼ì…
    # 14: TextEncodeAceStepAudio (tags, lyrics_strength)
    # 80: CLIPTextEncode (Negative)
    # 17: EmptyAceStepLatentAudio (seconds)
    # 52: KSampler (seed)
    # 78: SaveAudioMP3 (filename)

    # 1. ê¸ì • í”„ë¡¬í”„íŠ¸ & ê°€ì‚¬ ì–µì œ
    if "14" in graph:
        graph["14"]["inputs"]["tags"] = prompt
        graph["14"]["inputs"]["lyrics_strength"] = 0  # ê°€ì‚¬ ë°©ì§€ í•µì‹¬
        # lyrics ì…ë ¥ ëŠê¸° (ì•ˆì „ì¥ì¹˜) -> ë¹ˆ ë¬¸ìì—´ì„ ì£¼ëŠ” ë…¸ë“œê°€ ìˆë‹¤ë©´ ì—°ê²°, ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ë¹„ìš°ê¸°
        # ì—¬ê¸°ì„œëŠ” tagsì—ë§Œ ì˜ì¡´í•˜ê³  lyrics_strength=0ìœ¼ë¡œ ì œì–´

    # 2. ë¶€ì • í”„ë¡¬í”„íŠ¸ (ë³´ì»¬ ë°©ì§€)
    if "80" in graph:
        graph["80"]["inputs"]["text"] = negative_prompt

    # 3. ê¸¸ì´ ì„¤ì • (ì—¬ìœ ë¶„ 3ì´ˆ ì¶”ê°€)
    target_sec = math.ceil(duration_sec + 3.0)
    if "17" in graph:
        graph["17"]["inputs"]["seconds"] = target_sec

    # 4. ì‹œë“œ ëœë¤í™”
    if "52" in graph:
        graph["52"]["inputs"]["seed"] = random.randint(1, 2 ** 32)

    # 5. ì €ì¥ ê²½ë¡œ ì„¤ì •
    # ComfyUI output í´ë” ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
    # ì˜ˆ: "bgm/product_name_bgm" (í™•ì¥ìëŠ” SaveAudio ë…¸ë“œê°€ ë¶™ì„)
    file_prefix = f"bgm/shopping_bgm_{uuid.uuid4().hex[:6]}"
    if "78" in graph:
        graph["78"]["inputs"]["filename_prefix"] = file_prefix

    # ì‹¤í–‰
    try:
        res = _submit_and_wait_local(comfy_host, graph, timeout=600)  # ì˜¤ë””ì˜¤ ìƒì„±ì€ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŒ

        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (move to out_path)
        outputs = res.get("outputs", {})
        for nid, out_d in outputs.items():
            if "audio" in out_d:
                for item in out_d["audio"]:
                    fname = item["filename"]
                    # ComfyUI output í´ë”ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    params = {"filename": fname, "subfolder": item.get("subfolder", ""),
                              "type": item.get("type", "output")}
                    resp = requests.get(f"{comfy_host}/view", params=params)
                    if resp.status_code == 200:
                        ensure_dir(out_path.parent)
                        with open(out_path, "wb") as f:
                            f.write(resp.content)
                        return True
        return False

    except Exception as e:
        print(f"âŒ BGM ìƒì„± ì‹¤íŒ¨: {e}")
        return False


# -----------------------------------------------------------------------------
# 2. ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (2-Step: Z-Image -> Qwen)
# -----------------------------------------------------------------------------


def build_shopping_images_2step(
        video_json_path: str | Path,
        *,
        ui_width: int = 720,
        ui_height: int = 1280,
        steps: int = 28,
        skip_if_exists: bool = True,
        on_progress: Optional[Callable[[Dict], None]] = None
) -> None:
    """
    [ìµœì¢… ìˆ˜ì •] ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
    - ì›ì¹™: í”„ë¡¬í”„íŠ¸ëŠ” ë¬´ì¡°ê±´ 'video_shopping.json'(ì›ë³¸)ì—ì„œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - video.jsonì€ ìƒì„±ëœ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì €ì¥í•˜ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"\n======== [Image Build Start (Source: video_shopping.json)] ========")
    print(f"Target: {video_json_path}")

    vpath = Path(video_json_path)
    product_dir = vpath.parent
    imgs_dir = ensure_dir(product_dir / "imgs")

    # 1. ì œí’ˆ ì´ë¯¸ì§€ ì°¾ê¸°
    product_json_path = product_dir / "product.json"
    product_img_file = None
    if product_json_path.exists():
        try:
            pj = json.loads(product_json_path.read_text(encoding="utf-8"))
            if pj.get("image_file"):
                pi = product_dir / pj["image_file"]
                if pi.exists():
                    product_img_file = pi
        except:
            pass

    if product_img_file:
        print(f"âœ… Product Image Found: {product_img_file.name}")
    else:
        print(f"âŒ Product Image NOT FOUND! Step 2 will be skipped.")

    # 2. [í•µì‹¬] ì›ë³¸ ë°ì´í„°(video_shopping.json) ë¡œë“œ -> í”„ë¡¬í”„íŠ¸ì˜ ìœ ì¼í•œ ì¶œì²˜
    shopping_source_map = {}
    shop_json_path = product_dir / "video_shopping.json"

    if not shop_json_path.exists():
        print(f"âŒ Critical Error: video_shopping.json not found!")
        if on_progress: on_progress({"msg": "âŒ ì›ë³¸ ë°ì´í„°(video_shopping.json)ê°€ ì—†ìŠµë‹ˆë‹¤."})
        return

    try:
        shop_data = load_json(shop_json_path, {})
        shop_scenes = shop_data.get("scenes", [])

        # ID ë§¤í•‘ (001, 1, t_001 ë“± ë‹¤ì–‘í•œ í¬ë§· ëŒ€ì‘)
        for ss in shop_scenes:
            raw_id = str(ss.get("id", ""))
            # ê·¸ëŒ€ë¡œ ì €ì¥
            shopping_source_map[raw_id] = ss
            # ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ì €ì¥ (001 -> 1)
            if raw_id.isdigit():
                shopping_source_map[str(int(raw_id))] = ss
                shopping_source_map[f"t_{int(raw_id):03d}"] = ss
            # t_ ì œê±° ë²„ì „ ì €ì¥
            if raw_id.startswith("t_"):
                shopping_source_map[raw_id.replace("t_", "")] = ss

        print(f"âœ… Source Data Loaded: {len(shop_scenes)} scenes from video_shopping.json")
    except Exception as e:
        print(f"âŒ Failed to load video_shopping.json: {e}")
        return

    # 3. íƒ€ê²Ÿ ë°ì´í„°(video.json) ë¡œë“œ
    video_doc = load_json(vpath, {})
    scenes = video_doc.get("scenes", [])

    comfy_host = getattr(settings, "COMFY_HOST", "http://127.0.0.1:8188").rstrip("/")
    comfy_input_dir = Path(settings.COMFY_INPUT_DIR)
    comfy_input_dir.mkdir(parents=True, exist_ok=True)

    # -----------------------------------------------------------
    # Step 1: Z-Image Batch (ë² ì´ìŠ¤ ìƒì„±)
    # -----------------------------------------------------------
    if on_progress: on_progress({"msg": "=== [Step 1] ë² ì´ìŠ¤ ì´ë¯¸ì§€ ìƒì„± (Source ì°¸ì¡°) ==="})

    wf_z_path = Path(settings.JSONS_DIR) / "Z-Image-lora.json"
    if not wf_z_path.exists():
        wf_z_path = Path(r"C:\my_games\shorts_make\app\jsons\Z-Image-lora.json")

    if wf_z_path.exists():
        with open(wf_z_path, "r", encoding="utf-8") as f:
            graph_z_origin = json.load(f)

        for sc in scenes:
            sid = sc.get("id")
            temp_file = imgs_dir / f"temp_{sid}.png"

            # [í•µì‹¬] ë¬´ì¡°ê±´ ì›ë³¸(video_shopping.json)ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜´
            source_scene = shopping_source_map.get(sid)
            if not source_scene:
                # ë§¤í•‘ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ í‚¤ ì‹œë„
                if sid.startswith("t_"):
                    source_scene = shopping_source_map.get(sid.replace("t_", ""))
                    if not source_scene and sid.replace("t_", "").isdigit():
                        source_scene = shopping_source_map.get(str(int(sid.replace("t_", ""))))

            p1 = ""
            if source_scene:
                p1 = source_scene.get("prompt_img_1") or source_scene.get("prompt_img", "")

            if not p1:
                print(f"âš ï¸ [Step 1] No prompt in video_shopping.json for {sid} (Skipping)")
                continue

            # íŒŒì¼ ì¡´ì¬ ì‹œ ìŠ¤í‚µ
            if skip_if_exists and temp_file.exists() and temp_file.stat().st_size > 0:
                print(f"[Step 1] Skip existing: {sid}")
                continue

            if on_progress: on_progress({"msg": f"[Step 1] ë² ì´ìŠ¤ ìƒì„±: {sid}..."})
            print(f"[Step 1] Generating {sid} using prompt from Source...")

            graph = json.loads(json.dumps(graph_z_origin))
            for nid, node in graph.items():
                ctype = node.get("class_type", "")
                inputs = node.get("inputs", {})

                if ctype == "CLIPTextEncode" and nid == "6":
                    inputs["text"] = p1
                if "LatentImage" in ctype:
                    inputs["width"] = ui_width
                    inputs["height"] = ui_height
                if ctype == "KSampler" and "seed" in inputs:
                    inputs["seed"] = random.randint(1, 10 ** 9)
                    if "steps" in inputs: inputs["steps"] = steps
                if ctype == "PreviewImage":
                    node["class_type"] = "SaveImage"
                    node.setdefault("inputs", {})["filename_prefix"] = "Z_Base"

            try:
                res = _submit_and_wait_local(comfy_host, graph, on_progress=on_progress)
                outputs = res.get("outputs", {})
                found = False
                for _, out_d in outputs.items():
                    for img in out_d.get("images", []):
                        fname = img["filename"]
                        resp = requests.get(f"{comfy_host}/view", params={"filename": fname, "type": img["type"]})
                        with open(temp_file, "wb") as f:
                            f.write(resp.content)
                        found = True
                        break
                    if found: break
                if found:
                    print(f"âœ… [Step 1] Created: {temp_file.name}")
            except Exception as e:
                print(f"âŒ [Step 1] Error {sid}: {e}")

    # -----------------------------------------------------------
    # Step 2: Qwen Batch (ì œí’ˆ í•©ì„±)
    # -----------------------------------------------------------
    if on_progress: on_progress({"msg": "=== [Step 2] ì œí’ˆ í•©ì„± (Source ì°¸ì¡°) ==="})
    print("\n-------- Starting Step 2 (Qwen Edit) --------")

    wf_q_path = Path(settings.JSONS_DIR) / "QwenEdit2511-V1.json"
    if not wf_q_path.exists():
        wf_q_path = Path(r"C:\my_games\shorts_make\app\jsons\QwenEdit2511-V1.json")

    if not wf_q_path.exists() or not product_img_file:
        print("âŒ Step 2 Aborted: Missing workflow or product image.")
        return

    with open(wf_q_path, "r", encoding="utf-8") as f:
        graph_q_origin = json.load(f)

    prod_input_name = f"prod_{uuid.uuid4().hex[:6]}.png"
    shutil.copy2(product_img_file, comfy_input_dir / prod_input_name)

    for sc in scenes:
        sid = sc.get("id")
        final_file = imgs_dir / f"{sid}.png"
        temp_file = imgs_dir / f"temp_{sid}.png"

        # íŒŒì¼ ì¡´ì¬ ì‹œ ìŠ¤í‚µ
        if skip_if_exists and final_file.exists() and final_file.stat().st_size > 0:
            sc["img_file"] = str(final_file)
            print(f"[Step 2] Skip existing: {sid}")
            continue

        if not temp_file.exists():
            print(f"âš ï¸ [Step 2] Base image missing for {sid}. Skipping.")
            continue

        # [í•µì‹¬] ë¬´ì¡°ê±´ ì›ë³¸(video_shopping.json)ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜´
        source_scene = shopping_source_map.get(sid)
        if not source_scene:
            if sid.startswith("t_"):
                source_scene = shopping_source_map.get(sid.replace("t_", ""))
                if not source_scene and sid.replace("t_", "").isdigit():
                    source_scene = shopping_source_map.get(str(int(sid.replace("t_", ""))))

        raw_p2 = ""
        if source_scene:
            raw_p2 = source_scene.get("prompt_img_2") or ""

        if not raw_p2:
            print(f"  - No Prompt 2 in Source. Copying Step 1 image.")
            shutil.copy2(temp_file, final_file)
            sc["img_file"] = str(final_file)
            continue

        # [Auto-Fix] ì„ ìƒë‹˜ì´ ê°€ë¥´ì³ì£¼ì‹  ë¬¸ë²• ì ìš© ('from image 1' í•„ìˆ˜)
        p2_fixed = raw_p2
        if "from image 1" not in raw_p2.lower():
            pattern = re.compile(r"^(The|A|An)\s+([a-zA-Z0-9\s]+?)\s+(holding|has|is|with|placing|looking)",
                                 re.IGNORECASE)
            match = pattern.search(raw_p2)
            if match:
                p2_fixed = raw_p2.replace(match.group(2), f"{match.group(2)} from image 1", 1)
            else:
                p2_fixed = f"The subject from image 1 {raw_p2}"
            print(f"ğŸ”§ [Auto-Fix] {sid}: {p2_fixed}")
        else:
            print(f"ğŸ‘ [Prompt OK] {sid} (Source)")

        base_input_name = f"base_{sid}_{uuid.uuid4().hex[:6]}.png"
        shutil.copy2(temp_file, comfy_input_dir / base_input_name)

        graph = json.loads(json.dumps(graph_q_origin))

        if "9" in graph: graph["9"]["inputs"]["image"] = base_input_name
        if "32" in graph: graph["32"]["inputs"]["image"] = prod_input_name
        if "88" in graph: graph["88"]["inputs"]["value"] = p2_fixed

        for nid, node in graph.items():
            if node.get("class_type") == "PreviewImage":
                node["class_type"] = "SaveImage"
                node.setdefault("inputs", {})["filename_prefix"] = "ShopFinal"

        if on_progress: on_progress({"msg": f"[Step 2] í•©ì„± ì§„í–‰({sid})..."})
        try:
            res = _submit_and_wait_local(comfy_host, graph, on_progress=on_progress)
            outputs = res.get("outputs", {})
            found = False
            for _, out_d in outputs.items():
                for img in out_d.get("images", []):
                    fname = img["filename"]
                    resp = requests.get(f"{comfy_host}/view", params={"filename": fname, "type": img["type"]})
                    with open(final_file, "wb") as f:
                        f.write(resp.content)
                    sc["img_file"] = str(final_file)
                    found = True
                    break
                if found: break

            if found:
                print(f"âœ… Scene {sid} Synthesis Done.")
            else:
                print(f"âŒ Scene {sid} Failed (No output).")

        except Exception as e:
            print(f"âŒ Scene {sid} Error: {e}")

    # ìµœì¢… ê²°ê³¼ ì—…ë°ì´íŠ¸ (ì´ë¯¸ì§€ ê²½ë¡œ ë“±)
    try:
        with open(vpath, "w", encoding="utf-8") as f:
            json.dump(video_doc, f, indent=2, ensure_ascii=False)
    except:
        pass

    print("======== [Image Build End] ========\n")


# -----------------------------------------------------------------------------
# 3. ì˜µì…˜ ë°ì´í„° í´ë˜ìŠ¤
# -----------------------------------------------------------------------------
@dataclass
class BuildOptions:
    scene_count: int = 6
    style: str = "news_hook"
    hook_level: int = 3
    fps: int = 24
    allow_fallback_rule: bool = True


# -----------------------------------------------------------------------------
# 4. JSON ë¹Œë”
# -----------------------------------------------------------------------------

class ShoppingVideoJsonBuilder:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)
        self.ai = AI()

    def create_draft(self, product_dir: str | Path, product_data: Dict[str, Any], options: BuildOptions) -> Path:
        """
        [1ë‹¨ê³„] ê¸°íš ì´ˆì•ˆ ìƒì„±
        [ìˆ˜ì •] ì‹œì‘ë¶€í„° IDë¥¼ 't_001' í¬ë§·ìœ¼ë¡œ ê³ ì •í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ì „ì²´ í†µì¼ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
        """
        p_dir = Path(product_dir)
        vpath = p_dir / "video_shopping.json"

        product_name = product_data.get("product_name", "ìƒí’ˆëª… ì—†ìŒ")
        desc = product_data.get("description") or product_data.get("summary_source") or ""

        self.on_progress(f"[Draft] AI ê¸°íš ì‹œì‘ (ìƒí’ˆ: {product_name})...")

        # BGM ê°€ì´ë“œ
        bgm_guide = (
            "4. **Background Music (BGM)**: \n"
            "   - Design a prompt for audio generation.\n"
            "   - Format: 'instrumental, background music, [Mood], [Genre], [Instruments], [Tempo], [Energy]'.\n"
            "   - **NO Vocals**: Do not use words like song, singing, voice.\n"
            "   - **NO Model Name**: Do NOT include the word 'Ace-Step' in the output prompt.\n"
            "   - Example: 'instrumental, background music, bright, acoustic pop, guitar, piano, medium tempo, uplifting'."
        )

        # ì‹œê°ì  ì œì•½ì‚¬í•­
        visual_rules = (
            "5. **Visual Description Rules (Clean Prompt)**:\n"
            "   - Focus ONLY on the **Situation, Action, and Background**.\n"
            "   - **FORBIDDEN**: Do NOT describe specific product details like 'Logo', 'Text', 'QR Code', 'Specific Color', 'Label'.\n"
            "   - Bad: 'Product with a red logo spinning.' -> Good: 'The product spinning on the table.'\n"
            "   - Reason: The actual product image will be composited later, so text descriptions of details cause hallucinations."
        )

        system_prompt = (
            "ë‹¹ì‹ ì€ AI ì˜ìƒ ìƒì„±(I2V)ì„ ìœ„í•œ ìˆí¼ ê¸°íš ì „ë¬¸ê°€ì´ì ìŒì•… ê°ë…ì…ë‹ˆë‹¤. "
            "ìƒí’ˆì„ ë¶„ì„í•˜ì—¬ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            "**[í•„ìˆ˜ ì‹œê°í™” ê·œì¹™]**\n"
            "1. **One Scene = One Action**: í•œ ì¥ë©´ì—ëŠ” ì˜¤ì§ 'í•˜ë‚˜ì˜ ë™ì‘'ë§Œ ë¬˜ì‚¬.\n"
            "2. **No Split Screens**: ì „ì²´ í™”ë©´ êµ¬ì„±.\n"
            "3. **Focus on Impact**: ê²°ì •ì  ìˆœê°„ í¬ì°©.\n"
            f"{bgm_guide}\n"
            f"{visual_rules}"
        )

        user_prompt = f"""
        [ìƒí’ˆ ì •ë³´]
        - ìƒí’ˆëª…: {product_name}
        - ì„¤ëª…: {desc}

        [ì œì‘ ê°€ì´ë“œ]
        1. ì´ ì¥ë©´: {options.scene_count}ê°œ
        2. ìŠ¤íƒ€ì¼: {options.style}

        [ì¶œë ¥ í¬ë§· (JSON)]
        {{
            "meta": {{ 
                "title": "...", 
                "voice_gender": "female", 
                "character_prompt": "...", 
                "bgm_prompt": "instrumental, background music, ... (English Only)"
            }},
            "scenes": [
                {{
                    "id": "t_001",  <-- AIì—ê²Œ ì˜ˆì‹œë„ t_í¬ë§·ìœ¼ë¡œ ì œì‹œ
                    "banner": "...",
                    "prompt": "í™”ë©´ ë¬˜ì‚¬ (í•œê¸€, ì ˆëŒ€ ì‹œí€€ìŠ¤/ë‹¨ê³„ ë‚˜ì—´ ê¸ˆì§€, ë‹¨ì¼ ë™ì‘ ìœ„ì£¼, ë¡œê³ /í…ìŠ¤íŠ¸ ë¬˜ì‚¬ ê¸ˆì§€)",
                    "narration": "ì‹¤ì œ ì½ì„ ëŒ€ì‚¬ (ì§€ì‹œë¬¸ ì œì™¸)",
                    "sfx": "íš¨ê³¼ìŒ",
                    "voice_config": {{
                        "speed": 1.0, 
                        "emotion": {{ "neutral": 1.0, "happy": 0.0, "sad": 0.0, "disgust": 0.0, "fear": 0.0, "surprise": 0.0, "anger": 0.0, "other": 0.0 }}
                    }},
                    "subtitle": "..."
                }},
                ...
            ]
        }}
        """

        try:
            resp_text = self.ai.ask_smart(system_prompt, user_prompt, prefer="openai")
            data = self._safe_json_parse(resp_text)
        except Exception as e:
            self.on_progress(f"âŒ ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

        final_json = {
            "schema": "shopping_shorts_v2",
            "style": options.style,
            "product": product_data,
            "meta": data.get("meta", {}),
            "defaults": {"image": {"width": 720, "height": 1280}, "movie": {"fps": options.fps}},
            "audit": {"created_at": _now_str(), "step": "draft"},
            "scenes": []
        }

        imgs_dir = p_dir / "imgs"
        clips_dir = p_dir / "clips"
        voice_dir = p_dir / "voice"
        ensure_dir(imgs_dir)
        ensure_dir(clips_dir)
        ensure_dir(voice_dir)

        for idx, sc in enumerate(data.get("scenes", [])):
            # [í•µì‹¬ ìˆ˜ì •] AIê°€ 001ë¡œ ì£¼ë“  1ë¡œ ì£¼ë“ , ë¬´ì¡°ê±´ t_001 í¬ë§·ìœ¼ë¡œ ê°•ì œ ë³€í™˜
            # ì´ì œ video_shopping.json ë‹¨ê³„ë¶€í„° t_001ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
            sid = f"t_{idx + 1:03d}"

            new_scene = {
                "id": sid,
                "banner": sc.get("banner"),
                "prompt": sc.get("prompt", ""),
                "narration": sc.get("narration", ""),
                "sfx": sc.get("sfx", ""),
                "voice_config": sc.get("voice_config", {"speed": 1.0, "emotion": {"neutral": 1.0}}),
                "subtitle": sc.get("subtitle", ""),
                "seconds": 0,
                "prompt_img_1": "",
                "prompt_img_2": "",
                "prompt_movie": "",
                "prompt_negative": "",
                # ì´ë¯¸ì§€ ê²½ë¡œë„ t_001.png ë¡œ í†µì¼
                "img_file": str(imgs_dir / f"{sid}.png"),
                "movie_file": str(clips_dir / f"{sid}.mp4"),
                "voice_file": str(voice_dir / f"{sid}.wav")
            }
            final_json["scenes"].append(new_scene)

        save_json(vpath, final_json)
        self.on_progress(f"[Draft] ì´ˆì•ˆ ì™„ë£Œ. (ID í¬ë§·: t_001)")
        return vpath

    def enrich_video_json(self, video_json_path: str | Path, product_data: Dict[str, Any]) -> Path:
        """
        [2ë‹¨ê³„] ìƒì„¸í™” (ìŒì„± -> BGM -> ì˜ì–´ í”„ë¡¬í”„íŠ¸)
        - ID ë§¤ì¹­: t_001 í¬ë§· ì§€ì›
        - í”„ë¡¬í”„íŠ¸ 2: ë³µì¡í•œ ë¬˜ì‚¬ ì œê±°í•˜ê³  "Subject from image 1 ... object from image 2" ê³µì‹ ê°•ì œ
        """
        vpath = Path(video_json_path)
        p_dir = vpath.parent
        voice_dir = ensure_dir(p_dir / "voice")
        bgm_path = p_dir / "bgm.mp3"

        data = load_json(vpath, {})
        scenes = data.get("scenes", [])
        meta = data.get("meta", {})

        # ---------------------------------------------------------------------
        # 1. ìŒì„± ìƒì„± (ìœ ì§€)
        # ---------------------------------------------------------------------
        gender = meta.get("voice_gender", "female").lower()
        if "male" == gender:
            ref_voice = Path(r"C:\my_games\shorts_make\voice\ë‚¨ìì„±ìš°1.mp3")
        else:
            ref_voice = Path(r"C:\my_games\shorts_make\voice\ê¼¬ê¼¬ ìŒì„±.m4a")

        self.on_progress(f"[Enrich] 1/3ë‹¨ê³„: ìŒì„± ìƒì„± ({gender}) ë° ì •ë°€ ì¸¡ì •...")
        comfy_host = getattr(settings, "COMFY_HOST", "http://127.0.0.1:8188")
        total_dur = 0.0

        for sc in scenes:
            sid = sc["id"]
            config = _get_zonos_config(sc, self.ai)
            narr = sc.get("narration", "").strip()
            v_path = Path(sc.get("voice_file") or str(voice_dir / f"{sid}.wav"))

            if not narr:
                if sc.get("seconds", 0) == 0: sc["seconds"] = 3
                total_dur += sc["seconds"]
                continue

            if not v_path.exists() or v_path.stat().st_size == 0:
                self.on_progress(f"   ğŸ™ï¸ Scene {sid} ìŒì„± ìƒì„±...")
                success = generate_tts_zonos(narr, v_path, ref_voice, comfy_host, config)
                if not success:
                    sc["seconds"] = 4
                else:
                    final_dur = 0.0
                    for _ in range(5):
                        try:
                            d = get_audio_duration(str(v_path))
                            if d > 0:
                                final_dur = d
                                break
                        except:
                            pass
                        time.sleep(0.2)
                    sc["seconds"] = round(final_dur + 0.5, 2) if final_dur > 0 else 4

            total_dur += sc["seconds"]
            sc["voice_file"] = str(v_path)

        data.setdefault("meta", {})["total_duration"] = round(total_dur, 2)
        save_json(vpath, data)

        # ---------------------------------------------------------------------
        # 2. BGM ìƒì„± (ìœ ì§€)
        # ---------------------------------------------------------------------
        bgm_prompt = meta.get("bgm_prompt", "")
        if not bgm_prompt:
            bgm_prompt = "instrumental, background music, calm, minimal, piano, soft, loopable"
            meta["bgm_prompt"] = bgm_prompt

        if bgm_path.exists() and bgm_path.stat().st_size > 1024:
            self.on_progress(f"[Enrich] 2/3ë‹¨ê³„: BGM ì´ë¯¸ ì¡´ì¬ (ìŠ¤í‚µ).")
        else:
            self.on_progress(f"[Enrich] 2/3ë‹¨ê³„: BGM ìƒì„± ì¤‘...")
            generate_bgm_acestep(
                prompt=bgm_prompt,
                out_path=bgm_path,
                duration_sec=total_dur,
                comfy_host=comfy_host,
            )

        # ---------------------------------------------------------------------
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„¸í™” (ì‹¬í”Œí•˜ê³  ê°•ë ¥í•œ í•©ì„± ê³µì‹ ì ìš©)
        # ---------------------------------------------------------------------
        self.on_progress("[Enrich] 3/3ë‹¨ê³„: ë¹„ì£¼ì–¼ í”„ë¡¬í”„íŠ¸ ê³ ë„í™” (í•©ì„± ê³µì‹ ê°•ì œ)...")

        char_prompt = meta.get("character_prompt", "Young Korean model")
        if "male" == gender:
            gender_kw = "male, man"
        else:
            gender_kw = "female, woman"

        scene_texts = []
        for sc in scenes:
            scene_texts.append(f"- Scene {sc['id']} (Action): {sc.get('prompt')}")

        # [ìˆ˜ì •] ë³µì¡í•œ ì„¤ëª… ë‹¤ ë¹¼ê³  'ê³µì‹'ë§Œ ì§€í‚¤ë¼ê³  ëª…ë ¹
        sys_p = (
            "You are a prompt engineer for AI Image Compositing.\n"
            "Your Goal: Generate simple English prompts for 2-step generation.\n\n"
            "** STRICT RULES for `prompt_img_2` (The Paint/Composite Step) **\n"
            "You MUST use this exact sentence structure:\n"
            "\"[Subject] from image 1 [action] the object from image 2\"\n\n"
            "**Examples (Follow these exactly):**\n"
            "- \"The woman from image 1 holds the object from image 2 in her hand.\"\n"
            "- \"The man from image 1 looks at the object from image 2.\"\n"
            "- \"The table from image 1 has the object from image 2 placed on it.\"\n\n"
            "Do NOT use adjectives for the object (e.g. don't say 'red bottle', just say 'object from image 2').\n"
            "Do NOT add complex lighting or background details in prompt_img_2."
        )

        user_p = f"""
        Context: Character is "{char_prompt}" ({gender_kw}).

        Analyze these scenes and generate prompts:

        1. `prompt_img_1`: Describe the character/background. Leave space for the product. (e.g., "Woman extending empty hand")
        2. `prompt_img_2`: Apply the STRICT FORMULA: "[Subject] from image 1 ... object from image 2".
        3. `prompt_movie`: Simple camera movement (e.g., "Slow zoom in").

        [Scenes]
        {chr(10).join(scene_texts)}

        [Output JSON Format]
        {{
            "scenes": {{
                "t_001": {{ "prompt_img_1": "...", "prompt_img_2": "...", "prompt_negative": "...", "prompt_movie": "..." }},
                ...
            }}
        }}
        """

        try:
            resp = self.ai.ask_smart(sys_p, user_p, prefer="openai")
            enriched = self._safe_json_parse(resp)
            en_map = enriched.get("scenes", {})

            if isinstance(en_map, list):
                en_map = {f"t_{i + 1:03d}": item for i, item in enumerate(en_map)}

            for sc in scenes:
                sid = str(sc["id"])

                # ë§¤ì¹­ í›„ë³´ (t_001, 001 ë“±)
                candidates = [sid, sid.replace("t_", ""), sid.replace("t_", "").lstrip("0"), f"t_{sid}"]
                tgt = None

                # í‚¤ë¡œ ì°¾ê¸°
                for key in candidates:
                    if key in en_map:
                        tgt = en_map[key]
                        break

                # ê°’ìœ¼ë¡œ ì°¾ê¸°
                if not tgt:
                    for val in en_map.values():
                        if isinstance(val, dict) and str(val.get("id", "")) in candidates:
                            tgt = val
                            break

                if tgt:
                    sc["prompt_img_1"] = tgt.get("prompt_img_1", "")
                    sc["prompt_img_2"] = tgt.get("prompt_img_2", "")
                    sc["prompt_negative"] = tgt.get("prompt_negative", "")
                    sc["prompt_movie"] = tgt.get("prompt_movie", "")
                    sc["prompt_img"] = sc["prompt_img_1"]

            data["audit"]["enriched_at"] = _now_str()
            save_json(vpath, data)
            self.on_progress(f"[Enrich] ìƒì„¸í™” ì™„ë£Œ (í•©ì„± ê³µì‹ ì ìš©ë¨).")

        except Exception as e:
            self.on_progress(f"âŒ ìƒì„¸í™” ì‹¤íŒ¨: {e}")

        return vpath

    def _safe_json_parse(self, text: str) -> Dict:
        try:
            text = re.sub(r"```json", "", text, flags=re.I).replace("```", "")
            return json.loads(text)
        except:
            s, e = text.find("{"), text.rfind("}")
            if s != -1 and e != -1:
                return json.loads(text[s:e + 1])
            raise ValueError("Invalid JSON response")


# -----------------------------------------------------------------------------
# 5. ì´ë¯¸ì§€ ìƒì„±ê¸°
# -----------------------------------------------------------------------------
class ShoppingImageGenerator:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)

    def generate_images(self, video_json_path: str | Path, skip_if_exists: bool = True) -> None:
        def _cb(d):
            self.on_progress(d.get("msg", ""))

        # 1. í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì•ˆì „ì¥ì¹˜ í¬í•¨)
        img_size = settings.DEFAULT_IMG_SIZE
        width_val = img_size[0]  # ê°€ë¡œ
        height_val = img_size[1]  # ì„¸ë¡œ

        # 2. ìŠ¤í… ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        steps_val = settings.DEFAULT_T2I_STEPS

        try:
            build_shopping_images_2step(
                video_json_path=video_json_path,
                ui_width=width_val,  # settingsì—ì„œ ê°€ì ¸ì˜¨ ê°€ë¡œê°’
                ui_height=height_val,  # settingsì—ì„œ ê°€ì ¸ì˜¨ ì„¸ë¡œê°’
                steps=steps_val,  # settingsì—ì„œ ê°€ì ¸ì˜¨ ìŠ¤í… ìˆ˜
                on_progress=_cb
            )
        except Exception as e:
            self.on_progress(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {e}")
            raise e

# -----------------------------------------------------------------------------
# 6. ì˜ìƒ ìƒì„±/ë³‘í•©ê¸°
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# 6. ì˜ìƒ ìƒì„±/ë³‘í•©ê¸° (pydub ì˜¤ë””ì˜¤ ë¯¹ì‹± + ìë§‰ í•©ì„± ì¶”ê°€)
# -----------------------------------------------------------------------------
class ShoppingMovieGenerator:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)

    def generate_movies(self, video_json_path: str | Path, skip_if_exists: bool = True, fps: int = 24) -> None:
        vpath = Path(video_json_path)  # ì´ê²ƒì€ video_shopping.json ì…ë‹ˆë‹¤.
        project_dir = vpath.parent

        # [ì¤‘ìš”] I2V ì—”ì§„(build_shots_with_i2v)ì€ ë¬´ì¡°ê±´ 'video.json'ì„ ì°¾ìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ video_shopping.json ë‚´ìš©ì„ ë³µì‚¬í•œ 'ì„ì‹œ íŒŒì¼'ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
        temp_video_json = project_dir / "video.json"

        self.on_progress(f"[Movie] I2V ì¤€ë¹„: {vpath.name}")

        # 1. video_shopping.json ë‚´ìš©ì„ ì½ìŒ
        data = load_json(vpath, {})

        # 2. duration ì•ˆì „ì¥ì¹˜ (0ì´ë©´ ê¸°ë³¸ê°’ ë¶€ì—¬)
        for sc in data.get("scenes", []):
            if float(sc.get("duration", 0)) <= 0:
                sc["duration"] = float(sc.get("seconds", 4.0))

        # 3. ì„ì‹œ íŒŒì¼(video.json)ë¡œ ì €ì¥ -> ì—”ì§„ì´ ì´ê±¸ ì½ìŒ
        save_json(temp_video_json, data)

        def _cb(d):
            self.on_progress(d.get("msg", ""))

        try:
            # 4. ì—”ì§„ ì‹¤í–‰ (ì—”ì§„ì€ í´ë” ë‚´ì˜ video.jsonì„ ìë™ìœ¼ë¡œ ì°¾ìŒ)
            build_shots_with_i2v(str(project_dir), total_frames=0, ui_fps=fps, on_progress=_cb)
            self.on_progress("[Movie] ìƒì„± ì™„ë£Œ")
        finally:
            # 5. [í•„ìˆ˜] ì‘ì—…ì´ ëë‚˜ë©´ ì„ì‹œ íŒŒì¼(video.json)ì€ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ì‚­ì œ
            if temp_video_json.exists():
                try:
                    os.remove(temp_video_json)
                except:
                    pass

    def merge_movies(self, video_json_path: str | Path):
        """
        [3ë‹¨ê³„ ë³‘í•© í”„ë¡œì„¸ìŠ¤]
        1. Visual: concatenate_scene_clipsë¡œ ì˜ìƒ íŠ¸ë™ ë³‘í•© (temp_visual.mp4)
        2. Audio: pydubë¡œ BGM(20%, fadeout) + Voice(íƒ€ì„ìŠ¤íƒ¬í”„) ë¯¹ì‹± (mixed_audio.wav)
        3. Final: FFmpegë¡œ ìë§‰(.srt) ìƒì„± í›„ ì˜ìƒ+ì˜¤ë””ì˜¤+ìë§‰ í•©ì„± (final_shopping_video.mp4)
        """
        vpath = Path(video_json_path)
        project_dir = vpath.parent
        clips_dir = project_dir / "clips"
        voice_dir = project_dir / "voice"
        bgm_path = project_dir / "bgm.mp3"

        # ì¤‘ê°„ ì‚°ì¶œë¬¼ ê²½ë¡œ
        temp_visual_path = project_dir / "temp_visual_merged.mp4"
        mixed_audio_path = project_dir / "mixed_audio.wav"
        srt_path = project_dir / "subtitles.srt"
        final_output_path = project_dir / "final_shopping_video.mp4"

        ffmpeg_exe = getattr(settings, "FFMPEG_EXE", "ffmpeg")

        self.on_progress("[Merge] 1/3ë‹¨ê³„: ì˜ìƒ íŠ¸ë™ í•©ì¹˜ëŠ” ì¤‘...")
        data = load_json(vpath, {})
        scenes = data.get("scenes", [])

        # 1. ì˜ìƒ íŠ¸ë™ ë³‘í•© (Visual Only)
        clip_paths = []
        for sc in scenes:
            cpath = clips_dir / f"{sc['id']}.mp4"
            if cpath.exists():
                clip_paths.append(cpath)
            else:
                self.on_progress(f"âš ï¸ í´ë¦½ ëˆ„ë½ë¨: {cpath.name}")

        if not clip_paths:
            self.on_progress("âŒ ë³‘í•©í•  í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            # ê¸°ì¡´ í•¨ìˆ˜ í™œìš© (ì˜ìƒë§Œ ë¹ ë¥´ê²Œ ì´ì–´ë¶™ì„)
            concatenate_scene_clips(clip_paths, temp_visual_path, ffmpeg_exe)
        except Exception as e:
            self.on_progress(f"âŒ ì˜ìƒ íŠ¸ë™ ë³‘í•© ì‹¤íŒ¨: {e}")
            return

        # 2. ì˜¤ë””ì˜¤ ë¯¹ì‹± (BGM + Voice)
        self.on_progress("[Merge] 2/3ë‹¨ê³„: BGM ë° ë‚´ë ˆì´ì…˜ ë¯¹ì‹± ì¤‘...")
        try:
            total_duration = float(data.get("duration", 0))
            if total_duration <= 0:
                # ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´ ì”¬ í•©ê³„ë¡œ ê³„ì‚°
                total_duration = sum(float(s.get("end", 0)) - float(s.get("start", 0)) for s in scenes)

            self._mix_audio_with_pydub(scenes, bgm_path, voice_dir, total_duration, mixed_audio_path)
        except Exception as e:
            self.on_progress(f"âŒ ì˜¤ë””ì˜¤ ë¯¹ì‹± ì‹¤íŒ¨: {e}")
            return

        # 3. ìë§‰ ìƒì„± ë° ìµœì¢… ë Œë”ë§
        self.on_progress("[Merge] 3/3ë‹¨ê³„: ìë§‰ ìƒì„± ë° ìµœì¢… ë Œë”ë§...")
        try:
            # 3-1. SRT íŒŒì¼ ìƒì„±
            self._create_srt_file(scenes, srt_path)

            # 3-2. FFmpeg ìµœì¢… í•©ì„± (Visual + Mixed Audio + Subtitles)
            self._finalize_with_ffmpeg(
                ffmpeg_exe,
                temp_visual_path,
                mixed_audio_path,
                srt_path,
                final_output_path
            )

            self.on_progress(f"âœ… ìµœì¢… ë³‘í•© ì™„ë£Œ: {final_output_path.name}")

            # (ì„ íƒ) ì„ì‹œ íŒŒì¼ ì •ë¦¬
            # if temp_visual_path.exists(): os.remove(temp_visual_path)
            # if mixed_audio_path.exists(): os.remove(mixed_audio_path)
            # if srt_path.exists(): os.remove(srt_path)

        except Exception as e:
            self.on_progress(f"âŒ ìµœì¢… ë Œë”ë§ ì‹¤íŒ¨: {e}")

    # --- ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ ---

    def _mix_audio_with_pydub(self, scenes: List[Dict], bgm_path: Path, voice_dir: Path, total_dur_sec: float,
                              out_path: Path):
        """pydubë¥¼ ì‚¬ìš©í•˜ì—¬ BGM(20%ë³¼ë¥¨, í˜ì´ë“œì•„ì›ƒ)ê³¼ ë‚´ë ˆì´ì…˜ì„ ë¯¹ì‹±"""
        if AudioSegment is None:
            raise ImportError("pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # pydubëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‚¬ìš©
        total_ms = int(total_dur_sec * 1000)

        # 1. ë² ì´ìŠ¤ íŠ¸ë™ ìƒì„± (ë¬´ìŒ)
        final_mix = AudioSegment.silent(duration=total_ms)

        # 2. BGM ì²˜ë¦¬
        if bgm_path.exists():
            bgm = AudioSegment.from_file(str(bgm_path))

            # ê¸¸ì´ ë§ì¶”ê¸° (ì§§ìœ¼ë©´ ë£¨í”„)
            if len(bgm) < total_ms:
                loops = int(total_ms / len(bgm)) + 1
                bgm = bgm * loops

            bgm = bgm[:total_ms]  # ê¸¸ì´ ìë¥´ê¸°

            # ë³¼ë¥¨ 20%ë¡œ ì¤„ì´ê¸° (ì•½ -14dB)
            # 20 * log10(0.2) â‰ˆ -13.97 dB
            bgm = bgm - 14

            # ë§ˆì§€ë§‰ 2ì´ˆ í˜ì´ë“œ ì•„ì›ƒ
            bgm = bgm.fade_out(2000)

            # ë² ì´ìŠ¤ì— BGM í•©ì„±
            final_mix = final_mix.overlay(bgm)

        # 3. ë‚´ë ˆì´ì…˜(Voice) ë°°ì¹˜
        for sc in scenes:
            sid = sc.get("id")
            voice_file = sc.get("voice_file")

            # voice_file ê²½ë¡œê°€ ì ˆëŒ€ê²½ë¡œê°€ ì•„ë‹ ê²½ìš° voice_dir ê¸°ì¤€ íƒìƒ‰
            v_path = None
            if voice_file:
                if Path(voice_file).exists():
                    v_path = Path(voice_file)
                elif (voice_dir / Path(voice_file).name).exists():
                    v_path = voice_dir / Path(voice_file).name

            # ì—†ìœ¼ë©´ id ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ì°¾ê¸°
            if not v_path and (voice_dir / f"{sid}.wav").exists():
                v_path = voice_dir / f"{sid}.wav"

            if v_path and v_path.exists():
                voice_seg = AudioSegment.from_file(str(v_path))
                start_time = float(sc.get("start", 0))
                start_ms = int(start_time * 1000)

                # ë¯¹ì‹± (position ì¸ìë¡œ ìœ„ì¹˜ ì§€ì •)
                final_mix = final_mix.overlay(voice_seg, position=start_ms)

        # 4. ì €ì¥
        final_mix.export(str(out_path), format="wav")

    def _create_srt_file(self, scenes: List[Dict], srt_path: Path):
        """video.json ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìë§‰(SRT) íŒŒì¼ ìƒì„±"""

        def sec_to_srt_fmt(seconds: float) -> str:
            """ì´ˆ ë‹¨ìœ„ë¥¼ SRT ì‹œê°„ í¬ë§·(HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜"""
            millis = int((seconds % 1) * 1000)
            seconds = int(seconds)
            mins, secs = divmod(seconds, 60)
            hrs, mins = divmod(mins, 60)
            return f"{hrs:02}:{mins:02}:{secs:02},{millis:03}"

        with open(srt_path, "w", encoding="utf-8") as f:
            idx = 1
            for sc in scenes:
                text = sc.get("lyric") or sc.get("narration") or ""
                text = text.strip()
                if not text:
                    continue

                start = float(sc.get("start", 0))
                end = float(sc.get("end", 0))

                # ìë§‰ì´ ë„ˆë¬´ ì§§ê²Œ ì§€ë‚˜ê°€ëŠ” ê²ƒ ë°©ì§€
                if end - start < 0.5:
                    end = start + 2.0

                f.write(f"{idx}\n")
                f.write(f"{sec_to_srt_fmt(start)} --> {sec_to_srt_fmt(end)}\n")
                f.write(f"{text}\n\n")
                idx += 1

    def _finalize_with_ffmpeg(self, ffmpeg_exe: str, video_path: Path, audio_path: Path, srt_path: Path,
                              out_path: Path):
        """ì˜ìƒ, ì˜¤ë””ì˜¤, ìë§‰ì„ ìµœì¢… í•©ì„± (ì¬ì¸ì½”ë”© ë°œìƒ)"""

        # FFmpeg filterì—ì„œ ìœˆë„ìš° ê²½ë¡œ ì—­ìŠ¬ë˜ì‹œ(\)ëŠ” ì´ìŠ¤ì¼€ì´í”„ê°€ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ ìŠ¬ë˜ì‹œ(/)ë¡œ ë³€í™˜
        # ìë§‰ ê²½ë¡œ ì²˜ë¦¬
        srt_path_str = str(srt_path).replace("\\", "/").replace(":", "\\:")

        # ëª…ë ¹ì–´ êµ¬ì„±
        # -shortest: ì˜ìƒ/ì˜¤ë””ì˜¤ ì¤‘ ì§§ì€ ìª½ì— ë§ì¶° ëëƒ„
        # -c:v libx264: ìë§‰ í•©ì„±ì„ ìœ„í•´ ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”©
        # -c:a aac: ì˜¤ë””ì˜¤ ì¸ì½”ë”©
        # -vf subtitles=...: ìë§‰ í•„í„° (force_styleë¡œ í°íŠ¸ í¬ê¸°/ë°°ê²½ ë“± ìŠ¤íƒ€ì¼ ì§€ì • ê°€ëŠ¥)

        # ê¸°ë³¸ ìë§‰ ìŠ¤íƒ€ì¼: í°íŠ¸í¬ê¸° 20, êµµê²Œ, í°ìƒ‰ ê¸€ì”¨, ê²€ì€ í…Œë‘ë¦¬, í•˜ë‹¨ ì—¬ë°± 20
        sub_style = "Fontname=Arial,FontSize=20,Bold=1,PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,BorderStyle=1,Outline=2,Shadow=0,MarginV=30"

        cmd = [
            ffmpeg_exe,
            "-y",
            "-i", str(video_path),
            "-i", str(audio_path),
            "-vf", f"subtitles='{srt_path_str}':force_style='{sub_style}'",
            "-c:v", "libx264",
            "-preset", "fast",
            "-crf", "23",
            "-c:a", "aac",
            "-b:a", "192k",
            "-shortest",
            str(out_path)
        ]

        # ìœˆë„ìš°ì—ì„œ subprocess ì‹¤í–‰ ì‹œ ì½˜ì†” ì°½ ìˆ¨ê¸°ê¸°
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
            startupinfo=startupinfo
        )

        if result.returncode != 0:
            raise RuntimeError(f"FFmpeg Final Render Failed: {result.stderr}")


# -----------------------------------------------------------------------------
# 7. íŒŒì´í”„ë¼ì¸
# -----------------------------------------------------------------------------
class ShoppingShortsPipeline:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)

    def run_all(
            self,
            product_dir: str | Path,
            product_data: Dict[str, Any],
            options: Optional[BuildOptions] = None,
            build_json: bool = True,
            build_images: bool = True,
            build_movies: bool = True,
            merge: bool = True,
            skip_if_exists: bool = True,
    ) -> Path:
        options = options or BuildOptions()
        vpath = Path(product_dir) / "video_shopping.json"

        builder = ShoppingVideoJsonBuilder(self.on_progress)

        if build_json:
            if not vpath.exists():
                vpath = builder.create_draft(product_dir, product_data, options)
            builder.enrich_video_json(vpath, product_data)

        if build_images:
            img_gen = ShoppingImageGenerator(self.on_progress)
            img_gen.generate_images(vpath, skip_if_exists)

        if build_movies:
            mov_gen = ShoppingMovieGenerator(self.on_progress)
            mov_gen.generate_movies(vpath, skip_if_exists, fps=options.fps)

        if merge:
            mov_gen = ShoppingMovieGenerator(self.on_progress)
            mov_gen.merge_movies(vpath)

        return vpath


def convert_shopping_to_video_json_with_ai(
        project_dir: str,
        ai_client: Any = None,
        fps: int = 30,
        width: int = 1080,
        height: int = 1920,
        steps: int = 20,
        on_progress: Optional[Callable[[Dict[str, Any]], None]] = None
) -> str:
    """
    [ì‡¼í•‘->ì‡¼ì¸  ë³€í™˜ ìµœì¢…íŒ]
    - ì›ë³¸(video_shopping.json)ì˜ IDê°€ 't_001'ì´ë©´ ê·¸ëŒ€ë¡œ ê³„ìŠ¹í•©ë‹ˆë‹¤.
    - ì–µì§€ë¡œ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ IDë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì‚­ì œ/ìˆœì„œë³€ê²½ ëŒ€ì‘)
    """

    def _log(msg: str):
        if on_progress:
            on_progress({"msg": msg})
        print(f"[ShoppingConverter] {msg}")

    proj_path = Path(project_dir)
    src_json_path = proj_path / "video_shopping.json"
    dst_json_path = proj_path / "video.json"
    imgs_dir = proj_path / "imgs"

    if not src_json_path.exists():
        raise FileNotFoundError(f"video_shopping.jsonì´ ì—†ìŠµë‹ˆë‹¤: {src_json_path}")

    try:
        with open(src_json_path, "r", encoding="utf-8") as f:
            src_data = json.load(f)
    except Exception as e:
        raise ValueError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    _log("ë°ì´í„° êµ¬ì¡° ë³€í™˜ ì‹œì‘ (ID ê³„ìŠ¹ ëª¨ë“œ)...")

    prod = src_data.get("product", {})
    project_name = prod.get("product_name") or src_data.get("project_name", "Shopping Project")
    src_scenes = src_data.get("scenes", [])
    if not src_scenes:
        src_scenes = src_data.get("groups", [])

    new_scenes = []
    current_time = 0.0
    full_lyrics_parts = []

    for idx, sc in enumerate(src_scenes):
        # [í•µì‹¬ ìˆ˜ì •]
        # video_shopping.jsonì— ìˆëŠ” IDë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
        # ì´ˆì•ˆ ìƒì„± ë‹¨ê³„ì—ì„œ ì´ë¯¸ 't_001'ë¡œ ë§Œë“¤ì–´ì¡Œìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¨ë‹¤.
        original_id = str(sc.get("id", "")).strip()

        if original_id:
            scene_id = original_id
        else:
            # ë§Œì•½ êµ¬ë²„ì „ ë°ì´í„°ë¼ IDê°€ ì—†ë‹¤ë©´ ì•ˆì „ì¥ì¹˜ë¡œ ìƒì„±
            scene_id = f"t_{idx + 1:03d}"

        # ì´ë¯¸ì§€ëŠ” IDì™€ ë™ì¼í•œ ì´ë¦„ì˜ png íŒŒì¼
        target_img_name = f"{scene_id}.png"

        dur = float(sc.get("seconds") or sc.get("duration") or 4.0)
        start_t = current_time
        end_t = current_time + dur
        current_time = end_t

        narration = str(sc.get("narration") or sc.get("narration_text") or "")
        full_lyrics_parts.append(narration)

        new_scene = {
            "id": scene_id,  # t_001 ê·¸ëŒ€ë¡œ ìœ ì§€
            "section": "main",
            "start": round(start_t, 3),
            "end": round(end_t, 3),
            "duration": round(dur, 3),
            "img_file": str(imgs_dir / target_img_name),  # imgs/t_001.png
            "voice_file": sc.get("voice_file") or sc.get("audio_path") or "",
            "lyric": narration,
            "prompt": sc.get("prompt", ""),
            "prompt_movie": sc.get("prompt_movie", ""),
            "prompt_img": sc.get("prompt_img", ""),
            "prompt_negative": sc.get("prompt_negative", ""),
            "effect": [],
            "screen_transition": (idx == len(src_scenes) - 1)
        }
        new_scenes.append(new_scene)

    total_duration = current_time
    full_lyrics = "\n".join(full_lyrics_parts)

    video_data = {
        "title": project_name,
        "duration": round(total_duration, 3),
        "fps": fps,
        "lyrics": full_lyrics,
        "scenes": new_scenes,
        "defaults": {
            "movie": {"fps": fps, "target_fps": fps, "input_fps": fps},
            "image": {"width": width, "height": height, "fps": fps},
            "generator": {"steps": steps}
        },
        "audit": {
            "source": "shopping_converter_v2",
            "converted_at": str(datetime.datetime.now())
        }
    }

    with open(dst_json_path, "w", encoding="utf-8") as f:
        json.dump(video_data, f, indent=2, ensure_ascii=False)

    _log(f"video.json ì €ì¥ ì™„ë£Œ (ID: {scene_id} ë“±)")
    _log("AI ìƒì„¸í™” ì§„í–‰...")

    if ai_client:
        try:
            def ask_wrapper(sys_msg, user_msg):
                return ai_client.ask_smart(sys_msg, user_msg, prefer="openai")

            # IDê°€ t_001 í˜•ì‹ì´ë¯€ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  AIê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
            fill_prompt_movie_with_ai(
                str(dst_json_path.parent),
                ask_wrapper,
                log_fn=_log
            )
            _log("âœ… AI ìƒì„¸í™”(Segments/Prompts) ì™„ë£Œ.")
        except Exception as e:
            _log(f"âŒ AI ìƒì„¸í™” ì‹¤íŒ¨: {e}")

    return str(dst_json_path)
#