# -*- coding: utf-8 -*-
from __future__ import annotations

import datetime
import json
import re
import shutil
import os
import random
import requests
import subprocess
import time
import uuid
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
from pydub import AudioSegment

from app.utils import (
    AI,
    load_json,
    save_json,
    ensure_dir,
    _submit_and_wait as submit_and_wait,
    get_duration,

)
from app import settings
from app.video_build import build_shots_with_i2v, concatenate_scene_clips_final_av
from app.video_build import build_step1_zimage_base, build_step2_qwen_composite
from app.story_enrich import fill_prompt_movie_with_ai_shopping
def _now_str() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")



# -----------------------------------------------------------------------------
# 1. Zonos TTS ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------------------------
def _get_zonos_config(scene: Dict[str, Any], ai: AI = None) -> Dict[str, Any]:
    """
    ì”¬ ì •ë³´ì—ì„œ Zonosìš© ì„¤ì •(emotion, speed)ì„ ê°€ì ¸ì˜¤ê±°ë‚˜,
    ê¸°ì¡´ í…ìŠ¤íŠ¸ í¬ë§·([í†¤] ë‚´ìš© [íš¨ê³¼ìŒ])ì¼ ê²½ìš° AIë¡œ ë¶„ì„í•´ ë³€í™˜(Migration)í•œë‹¤.
    """
    # 1. ì´ë¯¸ ì„¤ì •ê°’ì´ ìˆìœ¼ë©´ ë°˜í™˜
    if "voice_config" in scene:
        return scene["voice_config"]

    # 2. ì„¤ì •ê°’ì´ ì—†ìœ¼ë©´ ë‚´ë ˆì´ì…˜ íŒŒì‹± ì‹œë„
    raw_narr = scene.get("narration", "").strip()
    if not raw_narr:
        return {"speed": 1.0, "emotion": {"neutral": 1.0}}

    # ì •ê·œì‹ìœ¼ë¡œ [í†¤] ë‚´ìš© [íš¨ê³¼ìŒ] ë¶„ë¦¬
    tone_match = re.match(r"^\[(.*?)\]\s*(.*)", raw_narr)

    tone_text = "calm"
    clean_text = raw_narr
    sfx_text = ""

    if tone_match:
        tone_text = tone_match.group(1).strip()
        remain = tone_match.group(2).strip()

        # ë’¤ìª½ SFX ì²´í¬
        sfx_match = re.search(r"\s*\[(.*?)\]$", remain)
        if sfx_match:
            sfx_text = sfx_match.group(1).strip()
            clean_text = remain[:sfx_match.start()].strip()
        else:
            clean_text = remain

        # ë”°ì˜´í‘œ ì œê±°
        clean_text = clean_text.strip("'").strip('"')

    # AIì—ê²Œ ìˆ˜ì¹˜ ë³€í™˜ ìš”ì²­ (Migration)
    if ai:
        try:
            sys_p = (
                "You are a Voice Director. Analyze the 'Tone Description' and convert it into Zonos TTS parameters.\n"
                "Output JSON only: { \"speed\": float(0.8-1.5), \"emotion\": { neutral, happy, sad, disgust, fear, surprise, anger, other } (sum approx 1.0) }"
            )
            user_p = f"Tone Description: \"{tone_text}\""
            res = ai.ask_smart(sys_p, user_p, prefer="openai")

            # JSON íŒŒì‹±
            if "```" in res: res = res.split("```")[1].replace("json", "")
            config = json.loads(res[res.find("{"):res.rfind("}") + 1])

            # ì”¬ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì˜êµ¬ ì €ì¥ìš©)
            scene["narration"] = clean_text
            scene["voice_config"] = config
            if sfx_text:
                scene["sfx"] = sfx_text

            return config
        except Exception as e:
            print(f"âš ï¸ í†¤ ë¶„ì„ ì‹¤íŒ¨: {e}")

    # ì‹¤íŒ¨/AI ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    return {"speed": 1.0, "emotion": {"neutral": 1.0}}


# [shopping_video_build.py]
# generate_tts_zonos í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.

def generate_tts_zonos(
        text: str,
        out_path: Path,
        ref_audio: Path,
        comfy_host: str = "http://127.0.0.1:8188",
        config: Dict[str, Any] = None
) -> bool:
    if not text:
        return False

    # 1. Zonos ì´ˆê¸° ì¡ìŒ ë°©ì§€ìš© '..' ìë™ ì¶”ê°€
    # (ì•ì— ì ì„ ì°ìœ¼ë©´ í˜¸í¡ìŒì´ ì¤„ì–´ë“œëŠ” íš¨ê³¼ê°€ ìˆì§€ë§Œ, ì™„ë²½í•˜ì§€ ì•Šì•„ íŠ¸ë¦¬ë°ë„ ë³‘í–‰í•©ë‹ˆë‹¤)
    tts_prompt_text = text
    if not tts_prompt_text.strip().startswith("."):
        tts_prompt_text = ".." + tts_prompt_text

    wf_path = Path(settings.JSONS_DIR) / "who_voice.json"
    if not wf_path.exists():
        wf_path = Path(r"C:\my_games\shorts_make\app\jsons\who_voice.json")

    if not wf_path.exists():
        print(f"âŒ TTS ì›Œí¬í”Œë¡œìš° ì—†ìŒ: {wf_path}")
        return False

    try:
        with open(wf_path, "r", encoding="utf-8") as f:
            graph = json.load(f)
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    if not ref_audio.exists():
        print(f"âŒ ì°¸ì¡° ì˜¤ë””ì˜¤ ì—†ìŒ: {ref_audio}")
        return False

    comfy_input_dir = Path(settings.COMFY_INPUT_DIR)
    comfy_input_dir.mkdir(parents=True, exist_ok=True)

    ref_copy_name = f"ref_{uuid.uuid4().hex[:8]}{ref_audio.suffix}"
    dst_ref = comfy_input_dir / ref_copy_name
    shutil.copy2(ref_audio, dst_ref)

    # 2. ë…¸ë“œ ê°’ ì„¤ì • (í…ìŠ¤íŠ¸, ì‹œë“œ, ì†ë„)
    found_gen = False
    for nid, node in graph.items():
        if "Zonos" in node.get("class_type", "") and "speech" in node.get("inputs", {}):
            node["inputs"]["speech"] = tts_prompt_text
            node["inputs"]["seed"] = random.randint(1, 2 ** 32)
            if config and "speed" in config:
                node["inputs"]["speed"] = config["speed"]
            found_gen = True
            break

    if not found_gen and "24" in graph:
        graph["24"]["inputs"]["speech"] = tts_prompt_text
        graph["24"]["inputs"]["seed"] = random.randint(1, 2 ** 32)
        if config and "speed" in config:
            graph["24"]["inputs"]["speed"] = config["speed"]

    # ê°ì • ì„¤ì •
    if config and "emotion" in config:
        emotions = config["emotion"]
        for nid, node in graph.items():
            if node.get("class_type") == "Zonos Emotion":
                for k, v in emotions.items():
                    if k in node["inputs"]:
                        node["inputs"][k] = v
                break

    # ì°¸ì¡° ì˜¤ë””ì˜¤ ì„¤ì •
    found_audio = False
    for nid, node in graph.items():
        if node.get("class_type") == "LoadAudio":
            node["inputs"]["audio"] = ref_copy_name
            found_audio = True
            break

    if not found_audio and "12" in graph:
        graph["12"]["inputs"]["audio"] = ref_copy_name

    try:
        res = submit_and_wait(comfy_host, graph, timeout=300)
        outputs = res.get("outputs", {})
        for nid, out_d in outputs.items():
            if "audio" in out_d:
                for item in out_d["audio"]:
                    fname = item["filename"]
                    params = {"filename": fname, "subfolder": item.get("subfolder", ""),
                              "type": item.get("type", "output")}
                    resp = requests.get(f"{comfy_host}/view", params=params)

                    if resp.status_code == 200:
                        ensure_dir(out_path.parent)
                        # ì¼ë‹¨ ì›ë³¸ ì €ì¥
                        with open(out_path, "wb") as f:
                            f.write(resp.content)

                        # [New] ì•ë¶€ë¶„ 0.2ì´ˆ(200ms) íŠ¸ë¦¬ë° ë¡œì§
                        try:
                            audio = AudioSegment.from_file(str(out_path))
                            # ê¸¸ì´ê°€ ì¶©ë¶„í•  ë•Œë§Œ ìë¦„
                            if len(audio) > 300:  # ìµœì†Œ 0.3ì´ˆëŠ” ë˜ì–´ì•¼ 0.2ì´ˆë¥¼ ìë¦„
                                trimmed = audio[350:]  # 200ms ë¶€í„° ëê¹Œì§€
                                trimmed.export(str(out_path), format="wav")
                                # print(f"âœ‚ï¸ Audio trimmed 0.2s: {out_path.name}")
                        except Exception as e:
                            print(f"âš ï¸ ì˜¤ë””ì˜¤ íŠ¸ë¦¬ë° ì‹¤íŒ¨ (ì›ë³¸ ìœ ì§€): {e}")

                        return True
        return False
    except Exception as e:
        print(f"TTS ìƒì„± ì‹¤íŒ¨: {e}")
        return False



# -----------------------------------------------------------------------------
# 1.5 BGM ìƒì„± í•¨ìˆ˜ (Ace-Step) - [New]
# -----------------------------------------------------------------------------
def generate_bgm_acestep(
        prompt: str,
        out_path: Path,
        duration_sec: float,
        comfy_host: str = "http://127.0.0.1:8188",
        negative_prompt: str = "vocal, vocals, singing, human voice, lyrics, rap, speech, chant, noise, distortion"
) -> bool:
    """
    [ì‡¼í•‘ ì „ìš©] Ace-Step ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ìŒ(BGM) ìƒì„±
    - ê°€ì‚¬ ìƒì„± ë°©ì§€(lyrics_strength=0) ë° Instrumental ê°•ì œ
    """
    if not prompt:
        return False

    # ì›Œí¬í”Œë¡œìš° ë¡œë“œ
    wf_path = Path(settings.JSONS_DIR) / "ace_step_1_t2mm.json"
    if not wf_path.exists():
        # ê¸°ë³¸ ê²½ë¡œ í´ë°±
        wf_path = Path(r"C:\my_games\shorts_make\app\jsons\ace_step_1_t2mm.json")

    if not wf_path.exists():
        print(f"âŒ Ace-Step ì›Œí¬í”Œë¡œìš° ì—†ìŒ: {wf_path}")
        return False

    try:
        with open(wf_path, "r", encoding="utf-8") as f:
            graph = json.load(f)
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # ë…¸ë“œ ë§¤í•‘ ë° ê°’ ì£¼ì…
    # 14: TextEncodeAceStepAudio (tags, lyrics_strength)
    # 80: CLIPTextEncode (Negative)
    # 17: EmptyAceStepLatentAudio (seconds)
    # 52: KSampler (seed)
    # 78: SaveAudioMP3 (filename)

    # 1. ê¸ì • í”„ë¡¬í”„íŠ¸ & ê°€ì‚¬ ì–µì œ
    if "14" in graph:
        graph["14"]["inputs"]["tags"] = prompt
        graph["14"]["inputs"]["lyrics_strength"] = 0  # ê°€ì‚¬ ë°©ì§€ í•µì‹¬
        # lyrics ì…ë ¥ ëŠê¸° (ì•ˆì „ì¥ì¹˜) -> ë¹ˆ ë¬¸ìì—´ì„ ì£¼ëŠ” ë…¸ë“œê°€ ìˆë‹¤ë©´ ì—°ê²°, ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ë¹„ìš°ê¸°
        # ì—¬ê¸°ì„œëŠ” tagsì—ë§Œ ì˜ì¡´í•˜ê³  lyrics_strength=0ìœ¼ë¡œ ì œì–´

    # 2. ë¶€ì • í”„ë¡¬í”„íŠ¸ (ë³´ì»¬ ë°©ì§€)
    if "80" in graph:
        graph["80"]["inputs"]["text"] = negative_prompt

    # 3. ê¸¸ì´ ì„¤ì • (ì—¬ìœ ë¶„ 3ì´ˆ ì¶”ê°€)
    target_sec = math.ceil(duration_sec + 3.0)
    if "17" in graph:
        graph["17"]["inputs"]["seconds"] = target_sec

    # 4. ì‹œë“œ ëœë¤í™”
    if "52" in graph:
        graph["52"]["inputs"]["seed"] = random.randint(1, 2 ** 32)

    # 5. ì €ì¥ ê²½ë¡œ ì„¤ì •
    # ComfyUI output í´ë” ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
    # ì˜ˆ: "bgm/product_name_bgm" (í™•ì¥ìëŠ” SaveAudio ë…¸ë“œê°€ ë¶™ì„)
    file_prefix = f"bgm/shopping_bgm_{uuid.uuid4().hex[:6]}"
    if "78" in graph:
        graph["78"]["inputs"]["filename_prefix"] = file_prefix

    # ì‹¤í–‰
    try:
        res = submit_and_wait(comfy_host, graph, timeout=600)  # ì˜¤ë””ì˜¤ ìƒì„±ì€ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŒ

        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (move to out_path)
        outputs = res.get("outputs", {})
        for nid, out_d in outputs.items():
            if "audio" in out_d:
                for item in out_d["audio"]:
                    fname = item["filename"]
                    # ComfyUI output í´ë”ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    params = {"filename": fname, "subfolder": item.get("subfolder", ""),
                              "type": item.get("type", "output")}
                    resp = requests.get(f"{comfy_host}/view", params=params)
                    if resp.status_code == 200:
                        ensure_dir(out_path.parent)
                        with open(out_path, "wb") as f:
                            f.write(resp.content)
                        return True
        return False

    except Exception as e:
        print(f"âŒ BGM ìƒì„± ì‹¤íŒ¨: {e}")
        return False


# -----------------------------------------------------------------------------
# 2. ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (2-Step: Z-Image -> Qwen)
# -----------------------------------------------------------------------------


def build_shopping_images_2step(
        video_json_path: str | object,
        *,
        source_json_path: str | object | None = None,
        product_image_path: str | object | None = None,
        ui_width: int | None = 720,
        ui_height: int | None = 1280,
        steps: int | None = 20,
        skip_if_exists: bool = True,
        on_progress: object = None,
) -> None:
    """
    ì‡¼í•‘ 2ë‹¨ê³„ ì´ë¯¸ì§€ ìƒì„± (ì •ìƒ ë™ì‘ ë²„ì „)
    - ê¸°ì¤€ JSON: video_shopping.json
    - Step1: prompt_img_1 ê¸°ë°˜ Z-Image â†’ imgs/temp_{sid}.png
    - Step2: prompt_img_2 ê¸°ë°˜ QwenEdit í•©ì„±
        * slot1 (image1) = temp_{sid}.png (ë°°ê²½/ì¸ë¬¼)
        * slot2 (image2) = product_image_path (ì œí’ˆ)
    """

    def _emit(msg: str) -> None:
        if not on_progress:
            return
        try:
            if callable(on_progress):
                on_progress({"stage": "debug", "msg": msg})
            elif isinstance(on_progress, dict) and callable(on_progress.get("callback")):
                on_progress["callback"]({"stage": "debug", "msg": msg})
        except Exception:
            pass

    if not video_json_path:
        _emit("[Image][ERR] video_json_path is None")
        return

    vpath = Path(str(video_json_path)).resolve()
    if not vpath.exists():
        _emit(f"[Image][ERR] video_shopping.json ì—†ìŒ: {vpath}")
        return

    if source_json_path is None:
        source_json_path = vpath

    proj_dir = vpath.parent
    imgs_dir = proj_dir / "imgs"

    doc = load_json(vpath, {}) or {}
    scenes = doc.get("scenes", []) or []

    prod_path = None
    if product_image_path:
        prod_path = Path(str(product_image_path)).resolve()
    _emit(f"[Image][DBG] product_image_path={str(prod_path) if prod_path else None} exists={prod_path.exists() if prod_path else False}")

    # -------------------------
    # Step1: Z-Image
    # -------------------------
    _emit(f"[Image] Step1(Z-Image) ì‹œì‘ ({ui_width}x{ui_height}, steps={steps})")
    try:
        build_step1_zimage_base(
            video_json_path=vpath,
            source_json_path=source_json_path,
            ui_width=ui_width,
            ui_height=ui_height,
            steps=steps,
            skip_if_exists=skip_if_exists,
            on_progress=on_progress,
        )
    except Exception as e:
        _emit(f"[Image][WARN] Step1 ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")

    src_map = {}
    try:
        p_src = Path(str(source_json_path)).resolve() if source_json_path else None
        if p_src and p_src.exists():
            sdoc = load_json(p_src, {}) or {}
            for s in (sdoc.get("scenes", []) or []):
                sid = str(s.get("id", "")).strip()
                if sid:
                    src_map[sid] = s
    except Exception as e:
        _emit(f"[Image][WARN] source_json_path ë§¤í•‘ ì‹¤íŒ¨: {e}")

    # -------------------------
    # Step2: Qwen Composite
    # -------------------------
    _emit("[Image] Step2(Qwen Composite) ì‹œì‘")

    for sc in scenes:
        sid = str(sc.get("id", "")).strip()
        if not sid:
            continue

        src_sc = src_map.get(sid, sc)

        p_edit = ""
        for k in ["prompt_img_2", "prompt_edit", "prompt"]:
            val = src_sc.get(k)
            if isinstance(val, str) and val.strip():
                p_edit = val.strip()
                break

        # [ìˆ˜ì •] ì‚¬ìš©ìê°€ ìš”ì²­í•œ ëŒ€ë¡œ slot1, slot2ë¡œ ëª…í™•íˆ êµ¬ë¶„
        # slot1 = Z-Image ê²°ê³¼ (ë°°ê²½/ì¸ë¬¼) -> ComfyUI Load Image 1ì— ë§¤í•‘ë¨ (ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ 0)
        step1_img_path = imgs_dir / f"temp_{sid}.png"
        slot1 = str(step1_img_path) if step1_img_path.exists() else None

        # slot2 = ì œí’ˆ ì´ë¯¸ì§€ -> ComfyUI Load Image 2ì— ë§¤í•‘ë¨ (ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ 1)
        slot2 = str(prod_path) if (prod_path and prod_path.exists()) else None

        _emit(f"[Image][DBG] sid={sid}")
        _emit(f"[Image][DBG]  slot1(z-image/image1)={slot1} exists={bool(slot1 and Path(slot1).exists())}")
        _emit(f"[Image][DBG]  slot2(product/image2)={slot2} exists={bool(slot2 and Path(slot2).exists())}")
        _emit(f"[Image][DBG]  prompt_img_2(actual)={p_edit}")

        try:
            build_step2_qwen_composite(
                video_json_path=vpath,
                source_json_path=source_json_path,
                workflow_path=None,
                ui_width=int(ui_width or 720),
                ui_height=int(ui_height or 1280),
                steps=int(steps or 20),
                edit_keys=["prompt_img_2", "prompt_edit", "prompt"],
                skip_if_exists=skip_if_exists,
                on_progress=on_progress,
                # [ìˆ˜ì •] slot1(Z-Image)ì„ ì²« ë²ˆì§¸, slot2(ì œí’ˆ)ë¥¼ ë‘ ë²ˆì§¸ë¡œ ì „ë‹¬
                slot_images=[slot1, slot2],
                # [ì¤‘ìš”] í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì”¬ IDë§Œ ì§€ì •í•˜ì—¬ í•´ë‹¹ ì”¬ë§Œ í•©ì„±í•˜ë„ë¡ ì œí•œ
                target_scene_ids=[sid]
            )
        except Exception as e:
            _emit(f"[Image][ERR] sid={sid} Step2 ì‹¤íŒ¨: {e}")
            continue

    _emit("[Image] ì‡¼í•‘ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")









# -----------------------------------------------------------------------------
# 3. ì˜µì…˜ ë°ì´í„° í´ë˜ìŠ¤
# -----------------------------------------------------------------------------
@dataclass
class BuildOptions:
    scene_count: int = 6
    style: str = "news_hook"
    hook_level: int = 3
    fps: int = 24
    allow_fallback_rule: bool = True


# -----------------------------------------------------------------------------
# 4. JSON ë¹Œë”
# -----------------------------------------------------------------------------

# ai ìƒì„¸í™”
class ShoppingVideoJsonBuilder:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)
        self.ai = AI()

    def create_draft(self, product_dir: str | Path, product_data: Dict[str, Any], options: BuildOptions) -> Path:
        """
        [1ë‹¨ê³„] ê¸°íš ì´ˆì•ˆ ìƒì„±
        - ì´ˆì•ˆ ìƒì„± ë‹¨ê³„ì—ì„œ prompt_img_1/2 + í•œê¸€ ë²„ì „(prompt_img_1_kor/2_kor)ê¹Œì§€ ê°™ì´ ìƒì„±
        - IDëŠ” t_001 í¬ë§·ìœ¼ë¡œ ê³ ì •
        """
        p_dir = Path(product_dir)
        vpath = p_dir / "video_shopping.json"

        product_name = product_data.get("product_name", "ìƒí’ˆëª… ì—†ìŒ")
        desc = product_data.get("description") or product_data.get("summary_source") or ""

        self.on_progress(f"[Draft] AI ê¸°íš ì‹œì‘ (ìƒí’ˆ: {product_name})...")

        # BGM ê°€ì´ë“œ
        bgm_guide = (
            "4. **Background Music (BGM)**: \n"
            "   - Design a prompt for audio generation.\n"
            "   - Format: 'instrumental, background music, [Mood], [Genre], [Instruments], [Tempo], [Energy]'.\n"
            "   - **NO Vocals**: Do not use words like song, singing, voice.\n"
            "   - **NO Model Name**: Do NOT include the word 'Ace-Step' in the output prompt.\n"
            "   - Example: 'instrumental, background music, bright, acoustic pop, guitar, piano, medium tempo, uplifting'."
        )

        # ì‹œê°ì  ì œì•½ì‚¬í•­
        visual_rules = (
            "5. **Visual Description Rules (Clean Prompt)**:\n"
            "   - Focus ONLY on the **Situation, Action, and Background**.\n"
            "   - **FORBIDDEN**: Do NOT describe specific product details like 'Logo', 'Text', 'QR Code', 'Specific Color', 'Label'.\n"
            "   - Bad: 'Product with a red logo spinning.' -> Good: 'The product spinning on the table.'\n"
            "   - Reason: The actual product image will be composited later, so text descriptions of details cause hallucinations."
        )

        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ê·œì¹™ (ì´ˆì•ˆ ë‹¨ê³„ì—ì„œ ë°”ë¡œ ìƒì„±)
        img_prompt_rules = (
            "6. **Image Prompt Rules (Two-Language Output)**:\n"
            "   - You MUST output BOTH Korean and English versions.\n"
            "   - `prompt_img_1_kor`: ì¥ë©´ì˜ ì¸ë¬¼/ë°°ê²½/ìƒí™©ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ë¬˜ì‚¬. (ì œí’ˆì˜ ë¡œê³ /í…ìŠ¤íŠ¸/ë¼ë²¨/ìƒ‰ìƒ ê°™ì€ ë””í…Œì¼ ê¸ˆì§€)\n"
            "   - `prompt_img_1`: ìœ„ `prompt_img_1_kor`ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ë¡œ ë²ˆì—­í•œ ë¬¸ì¥.\n"
            "   - `prompt_img_2_kor`: í•©ì„±(ì œí’ˆ ë¼ì›Œë„£ê¸°) ë‹¨ê³„ ì§€ì‹œë¥¼ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì‘ì„±.\n"
            "   - `prompt_img_2`: ë°˜ë“œì‹œ ì•„ë˜ ì˜ì–´ ê³ ì • ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì§€ì¼œ ì‘ì„±:\n"
            "       \"[Subject] from image 1 [action] the object from image 2\"\n"
            "     ì˜ˆ: \"The woman from image 1 holds the object from image 2 in her hand.\"\n"
            "   - `prompt_img_2`ì—ì„œëŠ” objectë¥¼ ì ˆëŒ€ êµ¬ì²´ì ìœ¼ë¡œ ë¬˜ì‚¬í•˜ì§€ ë§ ê²ƒ(ìƒ‰/ë¼ë²¨/í…ìŠ¤íŠ¸/ë¡œê³  ê¸ˆì§€).\n"
        )

        system_prompt = (
            "ë‹¹ì‹ ì€ AI ì˜ìƒ ìƒì„±(I2V)ì„ ìœ„í•œ ìˆí¼ ê¸°íš ì „ë¬¸ê°€ì´ì ìŒì•… ê°ë…ì…ë‹ˆë‹¤. "
            "ìƒí’ˆì„ ë¶„ì„í•˜ì—¬ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            "**[í•„ìˆ˜ ì‹œê°í™” ê·œì¹™]**\n"
            "1. **One Scene = One Action**: í•œ ì¥ë©´ì—ëŠ” ì˜¤ì§ 'í•˜ë‚˜ì˜ ë™ì‘'ë§Œ ë¬˜ì‚¬.\n"
            "2. **No Split Screens**: ì „ì²´ í™”ë©´ êµ¬ì„±.\n"
            "3. **Focus on Impact**: ê²°ì •ì  ìˆœê°„ í¬ì°©.\n"
            f"{bgm_guide}\n"
            f"{visual_rules}\n"
            f"{img_prompt_rules}\n\n"
            "ì¤‘ìš”: ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ. ì½”ë“œë¸”ë¡/ì„¤ëª…ë¬¸ ê¸ˆì§€."
        )

        user_prompt = f"""
        [ìƒí’ˆ ì •ë³´]
        - ìƒí’ˆëª…: {product_name}
        - ì„¤ëª…: {desc}

        [ì œì‘ ê°€ì´ë“œ]
        1. ì´ ì¥ë©´: {options.scene_count}ê°œ
        2. ìŠ¤íƒ€ì¼: {options.style}

        [ì¶œë ¥ í¬ë§· (JSON)]
        {{
            "meta": {{
                "title": "...",
                "voice_gender": "female",
                "character_prompt": "...",
                "bgm_prompt": "instrumental, background music, ... (English Only)"
            }},
            "scenes": [
                {{
                    "id": "t_001",
                    "banner": "...",
                    "prompt": "í™”ë©´ ë¬˜ì‚¬ (í•œê¸€, ì ˆëŒ€ ì‹œí€€ìŠ¤/ë‹¨ê³„ ë‚˜ì—´ ê¸ˆì§€, ë‹¨ì¼ ë™ì‘ ìœ„ì£¼, ë¡œê³ /í…ìŠ¤íŠ¸ ë¬˜ì‚¬ ê¸ˆì§€)",
                    "narration": "ì‹¤ì œ ì½ì„ ëŒ€ì‚¬ (ì§€ì‹œë¬¸ ì œì™¸)",
                    "sfx": "íš¨ê³¼ìŒ",
                    "voice_config": {{
                        "speed": 1.0,
                        "emotion": {{ "neutral": 1.0, "happy": 0.0, "sad": 0.0, "disgust": 0.0, "fear": 0.0, "surprise": 0.0, "anger": 0.0, "other": 0.0 }}
                    }},
                    "subtitle": "...",

                    "prompt_img_1_kor": "ì¥ë©´ì˜ ì¸ë¬¼/ë°°ê²½/ìƒí™© (í•œê¸€)",
                    "prompt_img_2_kor": "í•©ì„± ë‹¨ê³„ ì§€ì‹œ (í•œê¸€)",
                    "prompt_img_1": "English translation of prompt_img_1_kor",
                    "prompt_img_2": "MUST follow: \\"[Subject] from image 1 [action] the object from image 2\\"",

                    "prompt_movie": "Simple camera movement in English",
                    "prompt_negative": "negative prompt in English (short)"
                }},
                ...
            ]
        }}
        """

        try:
            resp_text = self.ai.ask_smart(system_prompt, user_prompt, prefer="openai")
            data = self._safe_json_parse(resp_text)
        except Exception as e:
            self.on_progress(f"âŒ ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

        final_json = {
            "schema": "shopping_shorts_v2",
            "style": options.style,
            "product": product_data,
            "meta": data.get("meta", {}),
            "defaults": {"image": {"width": 720, "height": 1280}, "movie": {"fps": options.fps}},
            "audit": {"created_at": _now_str(), "step": "draft"},
            "scenes": []
        }

        imgs_dir = p_dir / "imgs"
        clips_dir = p_dir / "clips"
        voice_dir = p_dir / "voice"
        ensure_dir(imgs_dir)
        ensure_dir(clips_dir)
        ensure_dir(voice_dir)

        for idx, sc in enumerate(data.get("scenes", [])):
            sid = f"t_{idx + 1:03d}"

            p1_kor = (sc.get("prompt_img_1_kor") or "").strip()
            p2_kor = (sc.get("prompt_img_2_kor") or "").strip()
            p1_eng = (sc.get("prompt_img_1") or "").strip()
            p2_eng = (sc.get("prompt_img_2") or "").strip()

            new_scene = {
                "id": sid,
                "banner": sc.get("banner"),
                "prompt": sc.get("prompt", ""),
                "narration": sc.get("narration", ""),
                "sfx": sc.get("sfx", ""),
                "voice_config": sc.get("voice_config", {"speed": 1.0, "emotion": {"neutral": 1.0}}),
                "subtitle": sc.get("subtitle", ""),
                "seconds": 0,

                # --- í•œê¸€/ì˜ë¬¸ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì €ì¥ ---
                "prompt_img_1_kor": p1_kor,
                "prompt_img_2_kor": p2_kor,
                "prompt_img_1": p1_eng,
                "prompt_img_2": p2_eng,

                "prompt_movie": (sc.get("prompt_movie") or ""),
                "prompt_negative": (sc.get("prompt_negative") or ""),

                # í˜¸í™˜ìš©: ê¸°ì¡´ ë¡œì§ì—ì„œ prompt_imgë¥¼ ì°¸ì¡°í•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ ìœ ì§€
                "prompt_img": p1_eng,

                # ì´ë¯¸ì§€/ì˜ìƒ/ë³´ì´ìŠ¤ ê²½ë¡œ
                "img_file": str(imgs_dir / f"{sid}.png"),
                "movie_file": str(clips_dir / f"{sid}.mp4"),
                "voice_file": str(voice_dir / f"{sid}.wav")
            }
            final_json["scenes"].append(new_scene)

        save_json(vpath, final_json)
        self.on_progress("[Draft] ì´ˆì•ˆ ì™„ë£Œ. (prompt_img_1/2 + _kor í¬í•¨)")
        return vpath

    def enrich_video_json(
            self,
            video_json_path: str | Path,
            product_data: Dict[str, Any],
            # [New] UI ì„¤ì •ê°’ì„ ë°›ì„ ì¸ì ì¶”ê°€
            ui_width: int = 720,
            ui_height: int = 1280,
            ui_fps: int = 24,
            ui_steps: int = 20
    ) -> Path:
        """
        [2ë‹¨ê³„] ìƒì„¸í™” (ìŒì„± -> BGM -> ì˜ì–´ í”„ë¡¬í”„íŠ¸)
        - ID ë§¤ì¹­: t_001 í¬ë§· ì§€ì›
        - í”„ë¡¬í”„íŠ¸ 2: ë³µì¡í•œ ë¬˜ì‚¬ ì œê±°í•˜ê³  "Subject from image 1 ... object from image 2" ê³µì‹ ê°•ì œ
        - [Fix] ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆì–´ë„ ê¸¸ì´(duration)ë¥¼ ê°•ì œ ì¬ì¸¡ì •í•˜ì—¬ 0ì´ˆ ë¬¸ì œ í•´ê²°
        - [Fix] UI ì„¤ì •(í•´ìƒë„, FPS ë“±)ì„ defaultsì— ì €ì¥
        """
        vpath = Path(video_json_path)
        p_dir = vpath.parent
        voice_dir = ensure_dir(p_dir / "voice")
        bgm_path = p_dir / "bgm.mp3"

        data = load_json(vpath, {})
        scenes = data.get("scenes", [])
        meta = data.get("meta", {})

        # [Fix] UI ì„¤ì •ê°’ ì €ì¥ (defaults ì—…ë°ì´íŠ¸)
        data.setdefault("defaults", {})
        data["defaults"].update({
            "image": {"width": ui_width, "height": ui_height, "fps": ui_fps},
            "movie": {"fps": ui_fps, "target_fps": ui_fps},
            "generator": {"steps": ui_steps}
        })

        # ---------------------------------------------------------------------
        # 1. ìŒì„± ìƒì„± (ìœ ì§€) ë° ì‹œê°„ ì¸¡ì •
        # ---------------------------------------------------------------------
        gender = meta.get("voice_gender", "female").lower()
        if "male" == gender:
            ref_voice = Path(r"C:\my_games\shorts_make\voice\ë‚¨ìì„±ìš°1.mp3")
        else:
            ref_voice = Path(r"C:\my_games\shorts_make\voice\ê¼¬ê¼¬ ìŒì„±.m4a")

        self.on_progress(f"[Enrich] 1/3ë‹¨ê³„: ìŒì„± ìƒì„± ({gender}) ë° ì •ë°€ ì¸¡ì •...")
        comfy_host = getattr(settings, "COMFY_HOST", "http://127.0.0.1:8188")
        total_dur = 0.0

        for sc in scenes:
            sid = sc["id"]
            config = _get_zonos_config(sc, self.ai)
            narr = sc.get("narration", "").strip()
            v_path = Path(sc.get("voice_file") or str(voice_dir / f"{sid}.wav"))

            # ë‚´ë ˆì´ì…˜ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ 3ì´ˆ
            if not narr:
                if sc.get("seconds", 0) == 0: sc["seconds"] = 3
                total_dur += sc["seconds"]
                continue

            # (A) íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
            if not v_path.exists() or v_path.stat().st_size == 0:
                self.on_progress(f"   ğŸ™ï¸ Scene {sid} ìŒì„± ìƒì„±...")
                success = generate_tts_zonos(narr, v_path, ref_voice, comfy_host, config)
                if not success:
                    # ì‹¤íŒ¨ ì‹œ ì„ì‹œ 4ì´ˆ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì¬ì‹œë„ ê°€ëŠ¥)
                    if sc.get("seconds", 0) <= 0: sc["seconds"] = 4.0

            # (B) [Fix] íŒŒì¼ì´ ì¡´ì¬í•˜ë©´(ìƒì„± ì§í›„ë“  ì›ë˜ ìˆì—ˆë“ ) ë¬´ì¡°ê±´ ê¸¸ì´ ì¸¡ì •
            if v_path.exists() and v_path.stat().st_size > 0:
                final_dur = 0.0
                # íŒŒì¼ ì“°ê¸° ì™„ë£Œ ëŒ€ê¸° ê²¸ ì¬ì‹œë„
                for _ in range(3):
                    try:
                        d = get_duration(str(v_path))
                        if d > 0:
                            final_dur = d
                            break
                    except:
                        pass
                    time.sleep(0.1)

                if final_dur > 0:
                    # [Fix] ì˜¤ë””ì˜¤ ê¸¸ì´ + 0.5ì´ˆ ì—¬ìœ 
                    sc["seconds"] = round(final_dur + 0.5, 2)
                else:
                    # ì¸¡ì • ì‹¤íŒ¨ ì‹œ ì•ˆì „ì¥ì¹˜
                    if sc.get("seconds", 0) <= 0: sc["seconds"] = 4.0

            total_dur += sc["seconds"]
            sc["voice_file"] = str(v_path)

        data.setdefault("meta", {})["total_duration"] = round(total_dur, 2)
        save_json(vpath, data)

        # ---------------------------------------------------------------------
        # 2. BGM ìƒì„± (ìœ ì§€)
        # ---------------------------------------------------------------------
        bgm_prompt = meta.get("bgm_prompt", "")
        if not bgm_prompt:
            bgm_prompt = "instrumental, background music, calm, minimal, piano, soft, loopable"
            meta["bgm_prompt"] = bgm_prompt

        if bgm_path.exists() and bgm_path.stat().st_size > 1024:
            self.on_progress(f"[Enrich] 2/3ë‹¨ê³„: BGM ì´ë¯¸ ì¡´ì¬ (ìŠ¤í‚µ).")
        else:
            self.on_progress(f"[Enrich] 2/3ë‹¨ê³„: BGM ìƒì„± ì¤‘...")
            generate_bgm_acestep(
                prompt=bgm_prompt,
                out_path=bgm_path,
                duration_sec=total_dur,
                comfy_host=comfy_host,
            )

        # ---------------------------------------------------------------------
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„¸í™” (ì‹¬í”Œí•˜ê³  ê°•ë ¥í•œ í•©ì„± ê³µì‹ ì ìš©)
        # ---------------------------------------------------------------------
        self.on_progress("[Enrich] 3/3ë‹¨ê³„: ë¹„ì£¼ì–¼ í”„ë¡¬í”„íŠ¸ ê³ ë„í™” (í•©ì„± ê³µì‹ ê°•ì œ)...")

        char_prompt = meta.get("character_prompt", "Young Korean model")
        if "male" == gender:
            gender_kw = "male, man"
        else:
            gender_kw = "female, woman"

        scene_texts = []
        for sc in scenes:
            scene_texts.append(f"- Scene {sc['id']} (Action): {sc.get('prompt')}")

        # [ìˆ˜ì •] ë³µì¡í•œ ì„¤ëª… ë‹¤ ë¹¼ê³  'ê³µì‹'ë§Œ ì§€í‚¤ë¼ê³  ëª…ë ¹
        sys_p = (
            "You are a prompt engineer for AI Image Compositing.\n"
            "Your Goal: Generate simple English prompts for 2-step generation.\n\n"
            "** STRICT RULES for `prompt_img_2` (The Paint/Composite Step) **\n"
            "You MUST use this exact sentence structure:\n"
            "\"[Subject] from image 1 [action] the object from image 2\"\n\n"
            "**Examples (Follow these exactly):**\n"
            "- \"The woman from image 1 holds the object from image 2 in her hand.\"\n"
            "- \"The man from image 1 looks at the object from image 2.\"\n"
            "- \"The table from image 1 has the object from image 2 placed on it.\"\n\n"
            "Do NOT use adjectives for the object (e.g. don't say 'red bottle', just say 'object from image 2').\n"
            "Do NOT add complex lighting or background details in prompt_img_2."
        )

        user_p = f"""
        Context: Character is "{char_prompt}" ({gender_kw}).

        Analyze these scenes and generate prompts:

        1. `prompt_img_1`: Describe the character/background. Leave space for the product. (e.g., "Woman extending empty hand")
        2. `prompt_img_2`: Apply the STRICT FORMULA: "[Subject] from image 1 ... object from image 2".
        3. `prompt_movie`: Simple camera movement (e.g., "Slow zoom in").

        [Scenes]
        {chr(10).join(scene_texts)}

        [Output JSON Format]
        {{
            "scenes": {{
                "t_001": {{ "prompt_img_1": "...", "prompt_img_2": "...", "prompt_negative": "...", "prompt_movie": "..." }},
                ...
            }}
        }}
        """

        try:
            resp = self.ai.ask_smart(sys_p, user_p, prefer="openai")
            enriched = self._safe_json_parse(resp)
            en_map = enriched.get("scenes", {})

            if isinstance(en_map, list):
                en_map = {f"t_{i + 1:03d}": item for i, item in enumerate(en_map)}

            for sc in scenes:
                sid = str(sc["id"])

                # ë§¤ì¹­ í›„ë³´ (t_001, 001 ë“±)
                candidates = [sid, sid.replace("t_", ""), sid.replace("t_", "").lstrip("0"), f"t_{sid}"]
                tgt = None

                # í‚¤ë¡œ ì°¾ê¸°
                for key in candidates:
                    if key in en_map:
                        tgt = en_map[key]
                        break

                # ê°’ìœ¼ë¡œ ì°¾ê¸°
                if not tgt:
                    for val in en_map.values():
                        if isinstance(val, dict) and str(val.get("id", "")) in candidates:
                            tgt = val
                            break

                if tgt:
                    sc["prompt_img_1"] = tgt.get("prompt_img_1", "")
                    sc["prompt_img_2"] = tgt.get("prompt_img_2", "")
                    sc["prompt_negative"] = tgt.get("prompt_negative", "")
                    sc["prompt_movie"] = tgt.get("prompt_movie", "")
                    sc["prompt_img"] = sc["prompt_img_1"]

            data["audit"]["enriched_at"] = _now_str()
            save_json(vpath, data)
            self.on_progress(f"[Enrich] ìƒì„¸í™” ì™„ë£Œ (í•©ì„± ê³µì‹ ì ìš©ë¨).")

        except Exception as e:
            self.on_progress(f"âŒ ìƒì„¸í™” ì‹¤íŒ¨: {e}")

        return vpath

    def _safe_json_parse(self, text: str) -> Dict:
        try:
            text = re.sub(r"```json", "", text, flags=re.I).replace("```", "")
            return json.loads(text)
        except:
            s, e = text.find("{"), text.rfind("}")
            if s != -1 and e != -1:
                return json.loads(text[s:e + 1])
            raise ValueError("Invalid JSON response")


# -----------------------------------------------------------------------------
# 5. ì´ë¯¸ì§€ ìƒì„±ê¸°
# -----------------------------------------------------------------------------
# class ShoppingImageGenerator:
#     def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
#         self.on_progress = on_progress or (lambda msg: None)
#
#     def generate_images(self, video_json_path: str | Path, skip_if_exists: bool = True) -> None:
#
#
#         def _cb(d):
#             self.on_progress(d.get("msg", ""))
#
#         vpath = Path(video_json_path).resolve()
#         proj_dir = vpath.parent
#
#         # 1. í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì•ˆì „ì¥ì¹˜ í¬í•¨)
#         img_size = settings.DEFAULT_IMG_SIZE
#         width_val = img_size[0]
#         height_val = img_size[1]
#
#         # 2. ìŠ¤í… ìˆ˜ ê°€ì ¸ì˜¤ê¸°
#         steps_val = settings.DEFAULT_T2I_STEPS
#
#         # 3. ì œí’ˆ ì´ë¯¸ì§€ ê²½ë¡œ ê³„ì‚° (shopping.py on_gen_images_clickedì™€ ë™ì¼ ë¡œì§)
#         prod_path: str | None = None
#         try:
#             doc = load_json(vpath, {}) or {}
#             product = doc.get("product") or {}
#             img_file = (product.get("image_file") or "").strip()
#             if img_file:
#                 cand = (proj_dir / img_file).resolve()
#                 if cand.exists():
#                     prod_path = str(cand)
#         except Exception:
#             prod_path = None
#
#         self.on_progress(f"[Image][DBG] product_image_path={prod_path}")
#
#         # 4. 2-Step ì´ë¯¸ì§€ ìƒì„± (ì œí’ˆ ì´ë¯¸ì§€ë¥¼ image2ë¡œ ê°•ì œ ì£¼ì…)
#         try:
#             build_shopping_images_2step(
#                 video_json_path=vpath,
#                 source_json_path=vpath,
#                 product_image_path=prod_path,
#                 ui_width=width_val,
#                 ui_height=height_val,
#                 steps=steps_val,
#                 skip_if_exists=skip_if_exists,
#                 on_progress=_cb,
#             )
#         except Exception as e:
#             self.on_progress(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {e}")
#             raise e


# -----------------------------------------------------------------------------
# 6. ì˜ìƒ ìƒì„±/ë³‘í•©ê¸°
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# 6. ì˜ìƒ ìƒì„±/ë³‘í•©ê¸° (pydub ì˜¤ë””ì˜¤ ë¯¹ì‹± + ìë§‰ í•©ì„± ì¶”ê°€)
# -----------------------------------------------------------------------------
class ShoppingMovieGenerator:
    def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
        self.on_progress = on_progress or (lambda msg: None)

    def generate_movies(self, video_json_path: str | Path, skip_if_exists: bool = True, fps: int = 24) -> None:
        vpath = Path(video_json_path)  # ì´ê²ƒì€ video_shopping.json ì…ë‹ˆë‹¤.
        project_dir = vpath.parent

        # [ì¤‘ìš”] I2V ì—”ì§„(build_shots_with_i2v)ì€ ë¬´ì¡°ê±´ 'video.json'ì„ ì°¾ìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ video_shopping.json ë‚´ìš©ì„ ë³µì‚¬í•œ 'ì„ì‹œ íŒŒì¼'ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
        temp_video_json = project_dir / "video.json"

        self.on_progress(f"[Movie] I2V ì¤€ë¹„: {vpath.name}")

        # 1. video_shopping.json ë‚´ìš©ì„ ì½ìŒ
        data = load_json(vpath, {})

        # 2. duration ì•ˆì „ì¥ì¹˜ (0ì´ë©´ ê¸°ë³¸ê°’ ë¶€ì—¬)
        for sc in data.get("scenes", []):
            if float(sc.get("duration", 0)) <= 0:
                sc["duration"] = float(sc.get("seconds", 4.0))

        # 3. ì„ì‹œ íŒŒì¼(video.json)ë¡œ ì €ì¥ -> ì—”ì§„ì´ ì´ê±¸ ì½ìŒ
        save_json(temp_video_json, data)

        def _cb(d):
            self.on_progress(d.get("msg", ""))

        try:
            # 4. ì—”ì§„ ì‹¤í–‰ (ì—”ì§„ì€ í´ë” ë‚´ì˜ video.jsonì„ ìë™ìœ¼ë¡œ ì°¾ìŒ)
            build_shots_with_i2v(str(project_dir), total_frames=0, ui_fps=fps, on_progress=_cb)
            self.on_progress("[Movie] ìƒì„± ì™„ë£Œ")
        finally:
            # 5. [í•„ìˆ˜] ì‘ì—…ì´ ëë‚˜ë©´ ì„ì‹œ íŒŒì¼(video.json)ì€ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ì‚­ì œ
            if temp_video_json.exists():
                try:
                    os.remove(temp_video_json)
                except:
                    pass

    def merge_movies(self, video_json_path: str | Path):
        """
        [ìµœì¢… ë³‘í•© - ì˜¤ë²„ë© ì—†ìŒ]
        - clips/*.mp4ë¥¼ ì‹¤ì œ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì¬íƒ€ì„ë¼ì¸ êµ¬ì„±
        - ë‚´ë ˆì´ì…˜/ìë§‰ì„ ê° ì”¬(í´ë¦½)ì˜ ì¤‘ì•™ì— ë°°ì¹˜
        - ë‚´ë ˆì´ì…˜ì´ ê¸¸ë©´ í•„ìš”í•œ ë§Œí¼ atempoë¡œ ì¤„ì„(ìƒí•œ 1.30)
        - ìë§‰ì€ padê°€ ì•„ë‹ˆë¼ fade-in/out(alpha)ë¡œ ì²˜ë¦¬
        - ìµœì¢… out: final_shopping_video.mp4
        """
        vpath = Path(video_json_path)
        project_dir = vpath.parent
        clips_dir = project_dir / "clips"
        bgm_path = project_dir / "bgm.mp3"

        final_output_path = project_dir / "final_shopping_video.mp4"
        ffmpeg_exe = getattr(settings, "FFMPEG_EXE", "ffmpeg")

        self.on_progress("[Merge] video.json ë¡œë“œ...")
        data = load_json(vpath, {})
        scenes = data.get("scenes", [])
        if not isinstance(scenes, list) or not scenes:
            self.on_progress("âŒ scenesê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
            return

        defaults = data.get("defaults", {}) if isinstance(data.get("defaults", {}), dict) else {}
        defaults_sub = defaults.get("subtitle", {}) if isinstance(defaults.get("subtitle", {}), dict) else {}

        font_family = str(defaults_sub.get("font_family") or getattr(settings, "DEFAULT_FONT_FAMILY", "Malgun Gothic"))
        title_size = int(defaults_sub.get("title_size") or getattr(settings, "DEFAULT_TITLE_FONT_SIZE", 55))
        narr_size = int(defaults_sub.get("narr_size") or getattr(settings, "DEFAULT_NARRATION_FONT_SIZE", 25))

        title_text = str(data.get("title") or "").strip()

        # ìë§‰ í˜ì´ë“œ(ì´ˆ) â€” ê¸°ë³¸ 0.25
        subtitle_fade_in_sec = float(defaults_sub.get("fade_in_sec") or 0.25)
        subtitle_fade_out_sec = float(defaults_sub.get("fade_out_sec") or 0.25)

        self.on_progress(
            f"[Merge] ì ìš©ê°’: font='{font_family}', title={title_size}, narr={narr_size}, "
            f"bgm={'YES' if bgm_path.exists() else 'NO'}, "
            f"subtitle_fade_in={subtitle_fade_in_sec:.2f}, subtitle_fade_out={subtitle_fade_out_sec:.2f}"
        )

        # 1) í´ë¦½ ìˆ˜ì§‘(ì”¬ ìˆœì„œëŒ€ë¡œ)
        clip_paths: List[Path] = []
        missing = False
        for sc in scenes:
            sid = str(sc.get("id") or "").strip()
            if not sid:
                continue
            cpath = clips_dir / f"{sid}.mp4"
            if cpath.exists() and cpath.stat().st_size > 0:
                clip_paths.append(cpath)
            else:
                self.on_progress(f"âš ï¸ í´ë¦½ ëˆ„ë½ë¨: {cpath.name}")
                missing = True

        if not clip_paths:
            self.on_progress("âŒ ë³‘í•©í•  í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if missing:
            self.on_progress("âš ï¸ ì¼ë¶€ ì”¬ í´ë¦½ì´ ëˆ„ë½ë˜ì–´, ì¡´ì¬í•˜ëŠ” í´ë¦½ë§Œìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")

        # 2) ìµœì¢… ë³‘í•©(ì˜ìƒ+ì˜¤ë””ì˜¤+ìë§‰ì„ í•œ ë²ˆì—)
        self.on_progress("[Merge] ìµœì¢… ë³‘í•©(ì‹¤ì¸¡ ê¸¸ì´ ê¸°ë°˜, ì˜¤ë²„ë© ì—†ìŒ, ìë§‰ í˜ì´ë“œ) ì‹œì‘...")
        try:
            concatenate_scene_clips_final_av(
                clip_paths=clip_paths,
                out_path=final_output_path,
                ffmpeg_exe=ffmpeg_exe,
                scenes=scenes,
                bgm_path=(bgm_path if bgm_path.exists() else None),
                bgm_volume=0.1,
                narration_volume=1.0,

                # âœ… ìµœì¢… ë³‘í•©ì€ ì˜¤ë²„ë©/usable ì¶•ì†Œ ì—†ìŒ
                pad_in_sec=0.0,
                pad_out_sec=0.0,

                # âœ… ìë§‰ í˜ì´ë“œëŠ” ë³„ë„ íŒŒë¼ë¯¸í„°ë¡œ
                subtitle_fade_in_sec=subtitle_fade_in_sec,
                subtitle_fade_out_sec=subtitle_fade_out_sec,

                subtitle_font=font_family,
                subtitle_fontsize=narr_size,
                subtitle_y="h-140",
                subtitle_box=True,
                subtitle_boxcolor="black@0.45",
                subtitle_boxborderw=18,
                title_text=title_text,
                title_fontsize=title_size,
                title_y="h*0.12",
                video_crf=18,
                video_preset="medium",
                audio_bitrate="192k",

                # âœ… ë³‘í•© ì¤‘ ìƒì„¸ ë¡œê·¸ë¥¼ ë¹„ë™ê¸°ì°½ì— ì¶œë ¥
                on_progress=self.on_progress,
            )

            try:
                save_json(vpath, data)
            except Exception:
                pass

            self.on_progress(f"âœ… ìµœì¢… ë³‘í•© ì™„ë£Œ: {final_output_path.name}")

        except Exception as e:
            self.on_progress(f"âŒ ìµœì¢… ë³‘í•© ì‹¤íŒ¨: {e}")

    def _finalize_with_ffmpeg(
            self,
            ffmpeg_exe: str,
            video_path: Path,
            audio_path: Path,
            srt_path: Path,
            out_path: Path,
            title_text: str = "",
            font_settings: dict = None
    ):
        """
        - drawtext(ì œëª©): fontfile ì‚¬ìš©
        - subtitles(SRT): fontsdir + force_style + original_sizeë¡œ í¬ê¸° ìŠ¤ì¼€ì¼ ë¬¸ì œ ë°©ì§€
        """

        if font_settings is None:
            font_settings = {}

        font_family = str(font_settings.get("family") or getattr(settings, "DEFAULT_FONT_FAMILY", "Malgun Gothic"))
        title_size = int(font_settings.get("title_size") or getattr(settings, "DEFAULT_TITLE_FONT_SIZE", 55))
        narr_size = int(font_settings.get("narr_size") or getattr(settings, "DEFAULT_NARRATION_FONT_SIZE", 25))
        sub_original_size = str(font_settings.get("sub_original_size") or "").strip()

        # drawtextìš© í°íŠ¸ íŒŒì¼ ë§¤í•‘
        font_file = "C:/Windows/Fonts/malgun.ttf"
        fam_lower = font_family.lower()

        if "êµ´ë¦¼" in fam_lower or "gulim" in fam_lower:
            font_file = "C:/Windows/Fonts/gulim.ttc"
        elif "ë°”íƒ•" in fam_lower or "batang" in fam_lower:
            font_file = "C:/Windows/Fonts/batang.ttc"
        elif "ë‹ì›€" in fam_lower or "dotum" in fam_lower:
            font_file = "C:/Windows/Fonts/dotum.ttc"
        elif "ê¶ì„œ" in fam_lower or "gungsuh" in fam_lower:
            font_file = "C:/Windows/Fonts/gungsuh.ttc"

        font_path_ffmpeg = font_file.replace("\\", "/").replace(":", "\\:")

        # subtitles í•„í„°ìš© ê²½ë¡œ/ìŠ¤íƒ€ì¼
        srt_path_str = str(srt_path).replace("\\", "/").replace(":", "\\:")
        fonts_dir = "C:/Windows/Fonts".replace("\\", "/").replace(":", "\\:")

        sub_style_raw = (
            f"FontName={font_family},FontSize={narr_size},Bold=1,"
            f"PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,"
            f"BorderStyle=1,Outline=2,Shadow=0,Alignment=2,MarginV=50"
        )

        # âœ… ë§¤ìš° ì¤‘ìš”: force_style ë‚´ë¶€ ì½¤ë§ˆëŠ” \, ë¡œ ì´ìŠ¤ì¼€ì´í”„ (í•„í„° ì²´ì¸ ì½¤ë§ˆì™€ ì¶©ëŒ ë°©ì§€)
        sub_style = sub_style_raw.replace(",", r"\,")

        filters: List[str] = []

        # (1) ìë§‰ í•„í„°: fontsdir + force_style + original_size
        # original_sizeë¥¼ ì£¼ë©´ libass ìŠ¤ì¼€ì¼ë§ì´ â€œì˜ìƒ ê¸°ì¤€â€ìœ¼ë¡œ ì¡í˜€ì„œ 25ê°€ 25ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.
        if sub_original_size:
            filters.append(
                f"subtitles='{srt_path_str}':fontsdir='{fonts_dir}':original_size={sub_original_size}:force_style='{sub_style}'"
            )
        else:
            filters.append(
                f"subtitles='{srt_path_str}':fontsdir='{fonts_dir}':force_style='{sub_style}'"
            )

        # (2) ì œëª© drawtext
        if title_text:
            safe_title = title_text.replace("'", r"\'").replace(":", r"\:")

            alpha_expr = "if(lt(t,1),0,if(lt(t,3),(t-1)/2,if(lt(t,4),1,if(lt(t,6),(6-t)/2,0))))"

            drawtext_filter = (
                f"drawtext=fontfile='{font_path_ffmpeg}':text='{safe_title}':"
                f"fontsize={title_size}:fontcolor=white:borderw=2:bordercolor=black:"
                f"x=(w-text_w)/2:y=h*0.15:"
                f"alpha='{alpha_expr}'"
            )
            filters.append(drawtext_filter)

        filter_complex = ",".join(filters)

        cmd = [
            ffmpeg_exe,
            "-y",
            "-i", str(video_path),
            "-i", str(audio_path),
            "-vf", filter_complex,
            "-c:v", "libx264",
            "-preset", "fast",
            "-crf", "23",
            "-c:a", "aac",
            "-b:a", "192k",
            "-shortest",
            str(out_path)
        ]

        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
            startupinfo=startupinfo
        )

        if result.returncode != 0:
            raise RuntimeError(f"FFmpeg ë Œë”ë§ ì‹¤íŒ¨:\n{result.stderr}")

    # --- ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ ---

    def _mix_audio_with_pydub(self, scenes: List[Dict], bgm_path: Path, voice_dir: Path, total_dur_sec: float,
                              out_path: Path):
        """pydubë¥¼ ì‚¬ìš©í•˜ì—¬ BGM(20%ë³¼ë¥¨, í˜ì´ë“œì•„ì›ƒ)ê³¼ ë‚´ë ˆì´ì…˜ì„ ë¯¹ì‹±"""
        if AudioSegment is None:
            raise ImportError("pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # pydubëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‚¬ìš©
        total_ms = int(total_dur_sec * 1000)

        # 1. ë² ì´ìŠ¤ íŠ¸ë™ ìƒì„± (ë¬´ìŒ)
        final_mix = AudioSegment.silent(duration=total_ms)

        # 2. BGM ì²˜ë¦¬
        if bgm_path.exists():
            bgm = AudioSegment.from_file(str(bgm_path))

            # ê¸¸ì´ ë§ì¶”ê¸° (ì§§ìœ¼ë©´ ë£¨í”„)
            if len(bgm) < total_ms:
                loops = int(total_ms / len(bgm)) + 1
                bgm = bgm * loops

            bgm = bgm[:total_ms]  # ê¸¸ì´ ìë¥´ê¸°

            # ë³¼ë¥¨ 20%ë¡œ ì¤„ì´ê¸° (ì•½ -14dB)
            # 20 * log10(0.2) â‰ˆ -13.97 dB
            bgm = bgm - 14

            # ë§ˆì§€ë§‰ 2ì´ˆ í˜ì´ë“œ ì•„ì›ƒ
            bgm = bgm.fade_out(2000)

            # ë² ì´ìŠ¤ì— BGM í•©ì„±
            final_mix = final_mix.overlay(bgm)

        # 3. ë‚´ë ˆì´ì…˜(Voice) ë°°ì¹˜
        for sc in scenes:
            sid = sc.get("id")
            voice_file = sc.get("voice_file")

            # voice_file ê²½ë¡œê°€ ì ˆëŒ€ê²½ë¡œê°€ ì•„ë‹ ê²½ìš° voice_dir ê¸°ì¤€ íƒìƒ‰
            v_path = None
            if voice_file:
                if Path(voice_file).exists():
                    v_path = Path(voice_file)
                elif (voice_dir / Path(voice_file).name).exists():
                    v_path = voice_dir / Path(voice_file).name

            # ì—†ìœ¼ë©´ id ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ì°¾ê¸°
            if not v_path and (voice_dir / f"{sid}.wav").exists():
                v_path = voice_dir / f"{sid}.wav"

            if v_path and v_path.exists():
                voice_seg = AudioSegment.from_file(str(v_path))
                start_time = float(sc.get("start", 0))
                start_ms = int(start_time * 1000)

                # ë¯¹ì‹± (position ì¸ìë¡œ ìœ„ì¹˜ ì§€ì •)
                final_mix = final_mix.overlay(voice_seg, position=start_ms)

        # 4. ì €ì¥
        final_mix.export(str(out_path), format="wav")

    def _create_srt_file(self, scenes: List[Dict], srt_path: Path):
        """video.json ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìë§‰(SRT) íŒŒì¼ ìƒì„±"""

        def sec_to_srt_fmt(seconds: float) -> str:
            """ì´ˆ ë‹¨ìœ„ë¥¼ SRT ì‹œê°„ í¬ë§·(HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜"""
            millis = int((seconds % 1) * 1000)
            seconds = int(seconds)
            mins, secs = divmod(seconds, 60)
            hrs, mins = divmod(mins, 60)
            return f"{hrs:02}:{mins:02}:{secs:02},{millis:03}"

        with open(srt_path, "w", encoding="utf-8") as f:
            idx = 1
            for sc in scenes:
                text = sc.get("lyric") or sc.get("narration") or ""
                text = text.strip()
                if not text:
                    continue

                start = float(sc.get("start", 0))
                end = float(sc.get("end", 0))

                # ìë§‰ì´ ë„ˆë¬´ ì§§ê²Œ ì§€ë‚˜ê°€ëŠ” ê²ƒ ë°©ì§€
                if end - start < 0.5:
                    end = start + 2.0

                f.write(f"{idx}\n")
                f.write(f"{sec_to_srt_fmt(start)} --> {sec_to_srt_fmt(end)}\n")
                f.write(f"{text}\n\n")
                idx += 1




# -----------------------------------------------------------------------------
# 7. íŒŒì´í”„ë¼ì¸
# -----------------------------------------------------------------------------
# class ShoppingShortsPipeline:
#     def __init__(self, on_progress: Optional[Callable[[str], None]] = None):
#         self.on_progress = on_progress or (lambda msg: None)
#
#     def run_all(
#             self,
#             product_dir: str | Path,
#             product_data: Dict[str, Any],
#             options: Optional[BuildOptions] = None,
#             build_json: bool = True,
#             build_images: bool = True,
#             build_movies: bool = True,
#             merge: bool = True,
#             skip_if_exists: bool = True,
#     ) -> Path:
#         options = options or BuildOptions()
#         vpath = Path(product_dir) / "video_shopping.json"
#
#         builder = ShoppingVideoJsonBuilder(self.on_progress)
#
#         if build_json:
#             if not vpath.exists():
#                 vpath = builder.create_draft(product_dir, product_data, options)
#             builder.enrich_video_json(vpath, product_data)
#
#         if build_images:
#             img_gen = ShoppingImageGenerator(self.on_progress)
#             img_gen.generate_images(vpath, skip_if_exists)
#
#         if build_movies:
#             mov_gen = ShoppingMovieGenerator(self.on_progress)
#             mov_gen.generate_movies(vpath, skip_if_exists, fps=options.fps)
#
#         if merge:
#             mov_gen = ShoppingMovieGenerator(self.on_progress)
#             mov_gen.merge_movies(vpath)
#
#         return vpath

# video_shopping_build.json ì—ì„œ ìŒì„±ê¸¸ì´ê°€ ê° 0.5ì´ˆ ì¶”ê°€ëœ ê²ƒì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´ (sc["seconds"] = round(final_dur + 0.5, 2)) ì´ ë¶€ë¶„ì„.
def convert_shopping_to_video_json_with_ai(
        project_dir: str,
        ai_client: Any = None,
        fps: int = 30,
        width: int = 1080,
        height: int = 1920,
        steps: int = 20,
        # [ì¶”ê°€] UIì—ì„œ ì „ë‹¬ë°›ì„ ì¸ìë“¤
        font_path: str = "",
        title_fontsize: int = 60,
        sub_fontsize: int = 40,
        on_progress: Optional[Callable[[Dict[str, Any]], None]] = None
) -> str:
    """
    [ì‡¼í•‘->ì‡¼ì¸  ë³€í™˜ ìµœì¢…íŒ]
    - video_shopping.json -> video.json êµ¬ì¡° ë³€í™˜
    - [ìˆ˜ì •] lyric í•„ë“œëŠ” 'narration' ê°’ì„ ìš°ì„  ì‚¬ìš©
    - [ìˆ˜ì •] UI ì„¤ì •(ê¸€ê¼´, í¬ê¸°)ì„ ì¸ìë¡œ ë°›ì•„ video.jsonì— ì €ì¥
    """
    import json
    import datetime
    from pathlib import Path
    from app.story_enrich import fill_prompt_movie_with_ai_shopping

    def _log(msg: str):
        if on_progress:
            on_progress({"msg": msg})
        print(f"[ShoppingConverter] {msg}")

    proj_path = Path(project_dir)
    src_json_path = proj_path / "video_shopping.json"
    dst_json_path = proj_path / "video.json"
    imgs_dir = proj_path / "imgs"

    if not src_json_path.exists():
        raise FileNotFoundError(f"video_shopping.jsonì´ ì—†ìŠµë‹ˆë‹¤: {src_json_path}")

    try:
        with open(src_json_path, "r", encoding="utf-8") as f:
            src_data = json.load(f)
    except Exception as e:
        raise ValueError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    _log(f"ë°ì´í„° êµ¬ì¡° ë³€í™˜ ì‹œì‘. (í•´ìƒë„: {width}x{height}, FPS: {fps})")

    prod = src_data.get("product", {})
    project_name = prod.get("product_name") or src_data.get("project_name", "Shopping Project")

    # ìºë¦­í„° ì •ë³´ ì¶”ì¶œ
    meta_info = src_data.get("meta", {})

    src_scenes = src_data.get("scenes", [])
    if not src_scenes:
        src_scenes = src_data.get("groups", [])

    new_scenes: List[Dict[str, Any]] = []
    current_time = 0.0
    full_lyrics_parts: List[str] = []

    for idx, sc in enumerate(src_scenes):
        original_id = str(sc.get("id", "")).strip()
        if original_id:
            scene_id = original_id
        else:
            scene_id = f"t_{idx + 1:03d}"

        target_img_name = f"{scene_id}.png"

        voice_file = sc.get("voice_file") or sc.get("audio_path") or ""
        voice_path_obj = None
        if voice_file:
            if Path(voice_file).is_absolute():
                voice_path_obj = Path(voice_file)
            else:
                voice_path_obj = proj_path / voice_file
            if not voice_path_obj.exists():
                voice_path_obj = None

        try:
            dur = float(sc.get("duration") or sc.get("seconds") or 4.0)
        except Exception:
            dur = 4.0
        if dur <= 0:
            dur = 4.0

        narration = str(sc.get("narration") or sc.get("narration_text") or sc.get("lyric") or "").strip()

        start_t = current_time
        end_t = current_time + dur
        current_time = end_t

        if narration:
            full_lyrics_parts.append(narration)

        new_scene = {
            "id": scene_id,
            "section": "main",
            "start": round(start_t, 3),
            "end": round(end_t, 3),
            "duration": round(dur, 3),
            "img_file": str(imgs_dir / target_img_name),
            "voice_file": str(voice_path_obj) if voice_path_obj else "",
            "lyric": narration,
            "prompt": sc.get("prompt", ""),
            "prompt_movie": sc.get("prompt_movie", ""),
            "prompt_img": sc.get("prompt_img", ""),
            "prompt_negative": sc.get("prompt_negative", ""),
            "effect": [],
            "screen_transition": (idx == len(src_scenes) - 1)
        }
        new_scenes.append(new_scene)

    total_duration = current_time
    full_lyrics = "\n".join(full_lyrics_parts)

    # [ì¤‘ìš”] 4ë²ˆ ê¸°ëŠ¥: UIì—ì„œ ë°›ì€ ê°’ ì €ì¥
    final_ui_prefs = {
        "font_path": str(font_path).strip(),
        "title_fontsize": int(title_fontsize) if title_fontsize > 0 else 60,
        "sub_fontsize": int(sub_fontsize) if sub_fontsize > 0 else 40
    }

    # ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ UI ì„¤ì • ë³‘í•©
    if dst_json_path.exists():
        try:
            with open(dst_json_path, "r", encoding="utf-8") as old_f:
                old_data = json.load(old_f)
                old_ui = old_data.get("defaults", {}).get("ui_prefs", {})
                if old_ui:
                    # ë¹ˆ ê°’ì´ë©´ ê¸°ì¡´ ê°’ ìœ ì§€, ê°’ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
                    if not final_ui_prefs["font_path"] and old_ui.get("font_path"):
                        final_ui_prefs["font_path"] = old_ui["font_path"]
                    # í°íŠ¸ í¬ê¸°ëŠ” UI ê°’ ìš°ì„  (ì´ë¯¸ ìœ„ì—ì„œ int ë³€í™˜ë¨)
                _log("ê¸°ì¡´ UI ì„¤ì •ì„ ë³‘í•©í–ˆìŠµë‹ˆë‹¤.")
        except Exception:
            pass

    video_data = {
        "title": project_name,
        "duration": round(total_duration, 3),
        "fps": fps,
        "lyrics": full_lyrics,
        "meta": meta_info,
        "scenes": new_scenes,
        "defaults": {
            "movie": {"fps": fps, "target_fps": fps, "input_fps": fps},
            "image": {"width": width, "height": height, "fps": fps},
            "generator": {"steps": steps},
            "ui_prefs": final_ui_prefs  # ì €ì¥
        },
        "audit": {
            "source": "shopping_converter_v2",
            "converted_at": str(datetime.datetime.now())
        }
    }

    # 1ì°¨ ì €ì¥
    with open(dst_json_path, "w", encoding="utf-8") as f:
        json.dump(video_data, f, indent=2, ensure_ascii=False)

    _log(f"video.json ê¸°ë³¸ ìƒì„± ì™„ë£Œ (ì´ {total_duration:.2f}ì´ˆ)")

    # AI ìƒì„¸í™”
    if ai_client:
        _log("AI ìƒì„¸í™” (Long-Take Prompt) ì§„í–‰...")
        try:
            def ask_wrapper(sys_msg, user_msg):
                return ai_client.ask_smart(sys_msg, user_msg, prefer="openai")

            def _trace_wrapper(tag, msg):
                _log(f"[{tag}] {msg}")

            video_data = fill_prompt_movie_with_ai_shopping(
                video_data,
                ask_wrapper,
                trace=_trace_wrapper
            )

            # AI ê²°ê³¼ ë°˜ì˜ í›„ ì¬ì €ì¥
            with open(dst_json_path, "w", encoding="utf-8") as f:
                json.dump(video_data, f, indent=2, ensure_ascii=False)

            _log("âœ… AI ìƒì„¸í™” ì™„ë£Œ.")
        except Exception as e:
            _log(f"âŒ AI ìƒì„¸í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    return str(dst_json_path)




def add_shopping_texts_with_drawtext(
        *,
        video_in_path: Path,
        video_json_path: Path,
        out_path: Path,
        ffmpeg_exe: str,
        font_family: str,
        title_fontsize: int,
        narr_fontsize: int,
) -> str:
    """
    Shopping íƒ­ ìµœì¢… ë Œë”ë§(drawtext):
    - subtitles(libass) ì‚¬ìš© ì•ˆ í•¨
    - drawtextë¡œ ì œëª© + ë‚´ë ˆì´ì…˜ì„ ì§ì ‘ í•˜ë“œì½”ë”©
    - ì œëª©: 1ì´ˆ ëŒ€ê¸° -> 2ì´ˆ fade in -> 1ì´ˆ ìœ ì§€ -> 2ì´ˆ fade out (ìƒë‹¨)
    - ë‚´ë ˆì´ì…˜: scene start~end ë™ì•ˆ í•˜ë‹¨ í‘œì‹œ

    [ì¤‘ìš”]
    - drawtextëŠ” ì¼ë¶€ í°íŠ¸(TTC/ë¹„íŠ¸ë§µ/ì»¬ë ‰ì…˜)ë¥¼ "1bpp"ë¡œ íŒì •í•˜ì—¬ ê±°ë¶€í•  ìˆ˜ ìˆìŒ.
    - ë”°ë¼ì„œ Windowsì—ì„œëŠ” ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” TTF(ì˜ˆ: malgun.ttf)ë¡œ ê°•ì œ.
    """

    video_data = load_json(video_json_path, {}) or {}
    scenes = video_data.get("scenes", [])

    meta = video_data.get("meta", {}) if isinstance(video_data.get("meta", {}), dict) else {}
    title = (meta.get("title") or video_data.get("title") or "").strip()

    # âœ… drawtext ì•ˆì • í°íŠ¸: malgun.ttfë¡œ ê°•ì œ (Shortsì—ì„œ ê²€ì¦ëœ ë°©ì‹)
    # êµ´ë¦¼/ë°”íƒ•/ë‹ì›€ ë“± TTC ê³„ì—´ì€ drawtextì—ì„œ 1bpp íŒì • ì´ìŠˆê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
    font_file = "C:/Windows/Fonts/malgun.ttf"
    font_path_ffmpeg = font_file.replace(os.path.sep, "/").replace(":", "\\:")

    filters: list[str] = []

    def _esc_ffmpeg_text(s: str) -> str:
        return (
            s.replace("\\", "\\\\")
             .replace(":", "\\:")
             .replace("'", "'\\\\''")
        )

    # 1) ì œëª© (í˜ì´ë“œ)
    if title:
        title_escaped = _esc_ffmpeg_text(title)
        alpha_expr = "if(lt(t,1),0,if(lt(t,3),(t-1)/2,if(lt(t,4),1,if(lt(t,6),(6-t)/2,0))))"

        filters.append(
            "drawtext="
            f"fontfile='{font_path_ffmpeg}':"
            f"text='{title_escaped}':"
            f"fontsize={int(title_fontsize)}:"
            "fontcolor=white:"
            "box=1:boxcolor=black@0.5:boxborderw=6:"
            "x=(w-text_w)/2:y=h*0.12:"
            f"alpha='{alpha_expr}'"
        )

    # 2) ë‚´ë ˆì´ì…˜(ì”¬ë³„)
    for sc in scenes:
        text = (sc.get("narration") or sc.get("lyric") or "").strip()
        if not text:
            continue

        start = float(sc.get("start", 0.0) or 0.0)
        end = float(sc.get("end", 0.0) or 0.0)
        if end <= start:
            continue

        text_escaped = _esc_ffmpeg_text(text).replace("\n", "\\n")

        filters.append(
            "drawtext="
            f"fontfile='{font_path_ffmpeg}':"
            f"text='{text_escaped}':"
            f"fontsize={int(narr_fontsize)}:"
            "fontcolor=white:"
            "box=1:boxcolor=black@0.5:boxborderw=5:"
            "x=(w-text_w)/2:y=h*0.82:"
            f"enable='between(t,{start},{end})'"
        )

    if not filters:
        shutil.copy2(str(video_in_path), str(out_path))
        return str(out_path)

    filter_complex = ",".join(filters)

    cmd = [
        ffmpeg_exe,
        "-y",
        "-i", str(video_in_path),
        "-vf", filter_complex,
        "-c:a", "copy",
        "-c:v", "libx264",
        "-preset", "fast",
        "-crf", "22",
        "-pix_fmt", "yuv420p",
        str(out_path)
    ]

    result = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore")

    if result.returncode != 0:
        raise RuntimeError(f"FFMPEG(drawtext) í…ìŠ¤íŠ¸ ì‚½ì… ì‹¤íŒ¨:\n{result.stderr}")

    return str(out_path)



#