


from googleapiclient.discovery import build
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import traceback
from googlesearch import search
import requests
from bs4 import BeautifulSoup
import json
import re
from app.blog_function import call_gemini  # â† ì´ë¯¸ ë„ˆ í”„ë¡œì íŠ¸ì— ìˆëŠ” ê·¸ í•¨ìˆ˜ ê²½ë¡œë¡œ ë§ì¶°
import variable as v_

def get_zum_ai_issue_trends():


    keywords = []
    try:
        options = Options()
        options.add_argument('--headless')  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get("https://zum.com/")

        # ìµœëŒ€ 300ì´ˆ ëŒ€ê¸°
        WebDriverWait(driver, 300).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.issue-word-list__keyword"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        keyword_spans = soup.select("span.issue-word-list__keyword")
        for span in keyword_spans:
            text = span.text.strip()
            if text:
                keywords.append(text)

        driver.quit()
    except Exception as e:
        print("âš ï¸ ZUM AI ì´ìŠˆ íŠ¸ë Œë“œ ì˜¤ë¥˜:", e)
    return keywords



    # ë‹µ ë°›ê¸°
    # ai_keywords = get_zum_ai_issue_trends()
    # print("â–¶ ZUM AI ì´ìŠˆ íŠ¸ë Œë“œ:")
    # for i, kw in enumerate(ai_keywords, 1):
    #     print(f"{i}. {kw}")


def get_google_trending_keywords():


    keywords = []

    try:
        options = Options()
        options.add_argument('--start-maximized')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        # options.add_argument('--headless')  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get("https://trends.google.co.kr/trends/trendingsearches/daily?geo=KR")

        # ìµœëŒ€ 300ì´ˆ ëŒ€ê¸°
        WebDriverWait(driver, 300).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.mZ3RIc'))
        )

        elements = driver.find_elements(By.CSS_SELECTOR, 'div.mZ3RIc')

        print("\nâ–¶ Google ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ:")
        for i, el in enumerate(elements[:20], 1):
            text = el.text.strip()
            if text:
                print(f"{i}. {text}")
                keywords.append(text)

        driver.quit()
    except Exception as e:
        print("âš ï¸ Google Trends ì˜¤ë¥˜:", e)

    return keywords


    # google_keywords = get_google_trending_keywords()
    # print("â–¶ Google ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ:")
    # for i, kw in enumerate(google_keywords, 1):
    #     print(f"{i}. {kw}")


def get_youtube_trending_titles(max_results=20, region_code="KR"):


    # ì¹´í…Œê³ ë¦¬ ID ë° ì„¤ëª… ì •ì˜
    category_map = {
        "22": "People & Blogs",
        "26": "Howto & Style",
        "24": "News & Politics"
    }

    all_titles = []

    try:
        youtube = build("youtube", "v3", developerKey=v_.my_google_custom_api)

        for category_id, category_name in category_map.items():
            print(f"\nğŸ“Œ [{category_name}] ì¹´í…Œê³ ë¦¬ ì˜ìƒ ì¶”ì¶œ ì¤‘...")

            response = youtube.videos().list(
                part="snippet",
                chart="mostPopular",
                regionCode=region_code,
                maxResults=max_results,
                videoCategoryId=category_id
            ).execute()

            category_titles = []
            for item in response.get("items", []):
                title = item["snippet"]["title"].strip()
                if title:
                    category_titles.append(title)
                    all_titles.append(title)

            for i, t in enumerate(category_titles, 1):
                print(f"{i}. {t}")

    except Exception as e:
        print("âš ï¸ YouTube API ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë”© ì˜¤ë¥˜:", e)

    return all_titles





    # yt_titles = get_youtube_trending_titles()
    # print("â–¶ YouTube ê¸‰ìƒìŠ¹ ì˜ìƒ ì œëª©:")
    # for i, title in enumerate(yt_titles, 1):
    #     print(f"{i}. {title}")




def fetch_health_titles(limit=30):
    import feedparser

    RSS_URL = "https://www.yna.co.kr/rss/health.xml"
    print("â–¶ ì—°í•©ë‰´ìŠ¤ ìƒí™œÂ·ê±´ê°• RSS ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    try:
        feed = feedparser.parse(RSS_URL)

        if not feed.entries:
            print("âŒ RSS ë°ì´í„° ì—†ìŒ")
            return []

        print(f"âœ… ì´ {len(feed.entries)}ê±´ ì¤‘ ìƒìœ„ {limit}ê°œ ì œëª© ì¶”ì¶œ:\n")

        titles = []
        for i, entry in enumerate(feed.entries[:limit], 1):
            try:
                title = entry.title.strip()
                if title:
                    print(f"{i}. {title}")
                    titles.append(title)
                else:
                    print(f"âš ï¸ {i}ë²ˆ í•­ëª©: ì œëª© ë¹„ì–´ ìˆìŒ")
            except Exception as e:
                print(f"âŒ {i}ë²ˆ í•­ëª©ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return titles
    except Exception as e:
        print(f"âŒ RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []


    #fetch_health_titles()




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ì‡¼í•‘ / ë„¤ì´ë²„ ë‰´ìŠ¤ ê´€ë ¨ ì‹ ê·œ ìˆ˜ì§‘ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ì‡¼í•‘ / ë„¤ì´ë²„ ë‰´ìŠ¤ ê´€ë ¨ ì‹ ê·œ ìˆ˜ì§‘ í•¨ìˆ˜ (Selenium ë²„ì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€



def _looks_like_product_keyword(text: str) -> bool:
    """
    ë„¤ì´ë²„ ì‡¼í•‘ BESTì—ì„œ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ ì¤‘
    'ìƒí’ˆ/í–‰ì‚¬/ì¹´í…Œê³ ë¦¬'ì²˜ëŸ¼ ë³´ì´ëŠ” ê²ƒë§Œ Trueë¡œ ë³¸ë‹¤.

    - ë„ˆë¬´ ì§§ì€ UI ë¬¸êµ¬, ê³„ì •/ì„¤ì •/ì•Œë¦¼/ë„¤ì´ë²„ ì„œë¹„ìŠ¤ ì´ë¦„ ë“±ì€ ì „ë¶€ False.
    - ì¡°ê¸ˆ ë¹¡ì„¸ê²Œ ê±°ë¥´ëŠ” ìª½ìœ¼ë¡œ ì„¤ê³„í•´ë„ ê´œì°®ë‹¤.
    """
    if not isinstance(text, str):
        return False

    t = text.strip()

    # ë„ˆë¬´ ì§§ìœ¼ë©´ ê±°ì˜ ë©”ë‰´/ë²„íŠ¼ í…ìŠ¤íŠ¸ì¼ ê°€ëŠ¥ì„±ì´ í¼
    if len(t) < 6:
        return False

    # ì™„ì „ UI/ì„œë¹„ìŠ¤ ì´ë¦„/ì„¤ì • ëŠë‚Œ ë‚˜ëŠ” ê²ƒë“¤ ë¨¼ì € ì»·
    ui_bad = [
        "ë³¸ë¬¸ìœ¼ë¡œ ë°”ë¡œê°€ê¸°", "ê³µìœ í•˜ê¸°", "ë§¨ìœ„ë¡œê°€ê¸°", "í™ˆì„ íƒë¨", "ì¹´í…Œê³ ë¦¬",
        "ì‚¬ìš©ì ë§í¬", "ë‚´ì •ë³´ ë³´ê¸°", "í”„ë¡œí•„ ì‚¬ì§„ ë³€ê²½", "ë¡œê·¸ì•„ì›ƒ",
        "@naver.com", "ë³´ì•ˆì„¤ì •", "ë‚´ì¸ì¦ì„œ", "ë‚´ í˜ì´í¬ì¸íŠ¸", "ë‚´ ë¸”ë¡œê·¸",
        "ê°€ì…í•œ ì¹´í˜", "í™˜ê²½ì„¤ì •", "ì „ì²´ ì•Œë¦¼", "ë‚´ ì•Œë¦¼ ì „ì²´ë³´ê¸°",
        "ì„œë¹„ìŠ¤ ë”ë³´ê¸°", "ì¦ê²¨ì°¾ëŠ” ì„œë¹„ìŠ¤", "ì¦ê²¨ì°¾ê¸° ì„¤ì •", "ì „ì²´ ì„œë¹„ìŠ¤ ë³´ê¸°",
        "ë°”ë¡œê°€ê¸° ì„¤ì •", "ì–´í•™ì‚¬ì „", "ì¸ê¸°/ì‹ ê·œì„œë¹„ìŠ¤", "ì´ˆê¸° ì„¤ì •ìœ¼ë¡œ ë³€ê²½",
        "NONE", "ë ˆì´ì–´ ì—´ê¸°", "ë„ì›€ë§ ì—´ê¸°", "ìƒì„¸ë³´ê¸°", "Best Keyword",
        "ì˜¤ëŠ˜ëë”œ", "ë² ìŠ¤íŠ¸ì„ íƒë¨",
    ]
    if t in ui_bad:
        return False

    # 1) ìˆ«ì + %, ì›, ìœ„, ë­í‚¹ ë“±: ë­í‚¹/ê°€ê²©/í• ì¸/ì„¸ì¼ í…ìŠ¤íŠ¸ì¼ ê°€ëŠ¥ì„±
    has_digit = any(ch.isdigit() for ch in t)
    if has_digit and any(tok in t for tok in ["%", "ì›", "ìœ„", "ë­í‚¹"]):
        return True

    # 2) í• ì¸/ì„¸ì¼/ìœ„í¬/ë¸Œëœë“œ ê°™ì€ ì‡¼í•‘ ëŠë‚Œ ë‹¨ì–´
    if any(kw in t for kw in ["í• ì¸", "ì„¸ì¼", "ìœ„í¬", "ë¸Œëœë“œ", "í–‰ì‚¬", "íŠ¹ê°€", "ë”œ", "ë°ì´"]):
        # ë‹¨, "ì¿ í°", "í˜œíƒ"ë§Œ ë‹¨ë…ìœ¼ë¡œ ìˆëŠ” ê±´ ë²„ë¦¼
        if t in ["ì¿ í°", "ì¿ í°í•¨", "ì¿ í°í˜œíƒ", "í˜œíƒ"]:
            return False
        return True

    # 3) ì¹´í…Œê³ ë¦¬/ìƒí’ˆêµ° ëŠë‚Œ (ì„ ê¸€ë¼ìŠ¤/íŒ¨ë”©/ì´ì–´í°/í‚¤ë³´ë“œ ë“±)
    category_keywords = [
        "ì„ ê¸€ë¼ìŠ¤", "ì•ˆê²½í…Œ", "íŒ¨ë”©", "ì í¼", "ë§¨íˆ¬ë§¨", "í›„ë“œ",
        "ì½”íŠ¸", "ìì¼“", "ì›í”¼ìŠ¤", "ì…”ì¸ ", "ë¸”ë¼ìš°ìŠ¤", "íŒ¬ì¸ ", "ë°”ì§€", "ìŠ¤ì»¤íŠ¸",
        "ìš´ë™í™”", "ìŠ¤ë‹ˆì»¤ì¦ˆ", "ìŠ¬ë¦¬í¼", "ìƒŒë“¤",
        "ì´ì–´í°", "í—¤ë“œí°", "ë…¸íŠ¸ë¶", "ëª¨ë‹ˆí„°", "í‚¤ë³´ë“œ", "ë§ˆìš°ìŠ¤",
        "ì²­ì†Œê¸°", "ì—ì–´ì»¨", "ê³µê¸°ì²­ì •ê¸°", "ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°",
        "ì—ì„¼ìŠ¤", "ì„¸ëŸ¼", "í¬ë¦¼", "í† ë„ˆ", "ë§ˆìŠ¤í¬íŒ©",
        "ì‚¬ë£Œ", "ê°„ì‹", "ê³ ì–‘ì´", "ê°•ì•„ì§€",
    ]
    if any(kw in t for kw in category_keywords):
        return True

    # 4) ê´„í˜¸ ì•ˆì— ë¸Œëœë“œ/ì˜µì…˜ + ìˆ«ìê°€ ì„ì—¬ ìˆìœ¼ë©´ ëŒ€ì¶© ìƒí’ˆ íƒ€ì´í‹€ ëŠë‚Œ
    if "(" in t and ")" in t and has_digit:
        return True

    # ê·¸ ì™¸ëŠ” ìƒí’ˆ/í–‰ì‚¬ í‚¤ì›Œë“œë¡œ ë³´ì§€ ì•ŠìŒ
    return False



def get_naver_shopping_best_topics(limit: int = 50):
    """
    Selenium + BeautifulSoupìœ¼ë¡œ ë„¤ì´ë²„ ì‡¼í•‘ BESTì—ì„œ
    í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ ì¤‘ 'ìƒí’ˆ/í‚¤ì›Œë“œ'ë¡œ ë³´ì´ëŠ” ë¬¸ìì—´ë§Œ ì¶”ì¶œí•œë‹¤.

    - í˜ì´ì§€ ì „ì²´ì—ì„œ a/span/strongì„ ê¸ë˜,
    - _looks_like_product_keyword() ë¡œ 'ìƒí’ˆ/í–‰ì‚¬/ì¹´í…Œê³ ë¦¬ ê°™ì€ ê²ƒ'ë§Œ ë‚¨ê¸´ë‹¤.
    """


    url = "https://shopping.naver.com/ns/home/best"

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )

    driver = None
    titles: list[str] = []

    try:
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=options
        )
        driver.get(url)

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "body")
                )
            )
        except Exception:
            # bodyë§Œ ê¸°ë‹¤ë¦¬ë˜, ì‹¤ì œ í…ìŠ¤íŠ¸ëŠ” page_sourceì—ì„œ íŒŒì‹±
            pass

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        candidates: list[str] = []

        # 1ì°¨: a/span/strong íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        for tag in soup.find_all(["a", "span", "strong"]):
            text = (tag.get_text() or "").strip()
            if not text:
                continue
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ì œì™¸
            if len(text) < 2 or len(text) > 80:
                continue

            # ë„¤ì´ë²„ ê³µí†µ UI/ë©”ë‰´/í‘¸í„° í…ìŠ¤íŠ¸ ë¹ ë¥´ê²Œ ì»·
            bad_keywords = [
                "ë„¤ì´ë²„", "ë¡œê·¸ì¸", "ì‡¼í•‘", "ì¥ë°”êµ¬ë‹ˆ", "ê³ ê°ì„¼í„°",
                "ë§ˆì´ì¿ í°", "ì˜¤ëŠ˜ ë³¸ ìƒí’ˆ", "ê²€ìƒ‰", "ì…ì ë¬¸ì˜", "ì´ìš©ì•½ê´€",
                "ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨", "ë³¸ë¬¸ ë°”ë¡œê°€ê¸°", "ë‹«ê¸°", "ì„œë¹„ìŠ¤", "ì„¤ì •",
                "í”„ë¡œí•„", "ì•Œë¦¼", "ì¶”ê°€í•´ ë³´ì„¸ìš”", "ì¦ê²¨ì°¾ê¸°", "ì „ì²´ë³´ê¸°",
            ]
            if any(bad in text for bad in bad_keywords):
                continue

            candidates.append(text)

        # ë””ë²„ê·¸ìš©: í›„ë³´ ê°œìˆ˜ ì°ê¸°
        print(f"ğŸ” [NAVER SHOPPING BEST] RAW í›„ë³´ ê°œìˆ˜: {len(candidates)}")

        # 2ì°¨: ì¤‘ë³µ ì œê±° + 'ìƒí’ˆ/í–‰ì‚¬ì²˜ëŸ¼ ë³´ì´ëŠ” ê²ƒ'ë§Œ ë‚¨ê¸°ê³  ìƒìœ„ limitê°œ
        seen = set()
        for t in candidates:
            if t in seen:
                continue
            seen.add(t)

            # ğŸ”¹ ìƒí’ˆ/í–‰ì‚¬/ì¹´í…Œê³ ë¦¬ ëŠë‚Œì´ ì•„ë‹ˆë©´ ê³¼ê°íˆ ë²„ë¦¼
            if not _looks_like_product_keyword(t):
                continue

            titles.append(t)
            if len(titles) >= limit:
                break

        print(f"âœ… [NAVER SHOPPING BEST] í•„í„° í›„ ì¶”ì¶œ: {len(titles)}ê°œ")

    except Exception as e:
        print(f"âš  ë„¤ì´ë²„ ì‡¼í•‘ BEST Selenium í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    finally:
        if driver is not None:
            driver.quit()

    return titles



def _extract_naver_news_titles_common(url: str, limit: int = 50):
    """
    Selenium + BeautifulSoup ê¸°ë°˜ ë„¤ì´ë²„ ë‰´ìŠ¤ ê³µí†µ íŒŒì„œ.
    - ë­í‚¹/ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì œëª© a[href*='/read?']ë¥¼ ì¶”ì¶œí•œë‹¤.
    """


    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )

    driver = None
    result: list[str] = []

    try:
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=options
        )
        driver.get(url)

        # ê¸°ì‚¬ ë§í¬ê°€ ë“±ì¥í•  ë•Œê¹Œì§€ ëŒ€ê¸° (ëŒ€ëµì ì¸ ì¡°ê±´)
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "a[href*='/read?']")
                )
            )
        except Exception:
            # ê·¸ë˜ë„ page_source ì „ì²´ì—ì„œ í•œ ë²ˆ ë” ì‹œë„
            pass

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        anchors = soup.select("a[href*='/read?']")
        titles_raw: list[str] = []
        for a in anchors:
            text = (a.get_text() or "").strip()
            if not text:
                continue
            if len(text) < 8 or len(text) > 80:
                continue
            titles_raw.append(text)

        # ì¤‘ë³µ ì œê±° + limit
        seen = set()
        for t in titles_raw:
            if t in seen:
                continue
            seen.add(t)
            result.append(t)
            if len(result) >= limit:
                break

        print(f"âœ… [NAVER NEWS] {url} ì œëª© {len(result)}ê°œ ì¶”ì¶œ")

    except Exception as e:
        print(f"âš  ë„¤ì´ë²„ ë‰´ìŠ¤ Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")

    finally:
        if driver is not None:
            driver.quit()

    return result


def get_naver_news_ranking_titles(limit: int = 50):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë­í‚¹ (ì¸ê¸° ê¸°ì‚¬)ì—ì„œ ì œëª©ì„ ìˆ˜ì§‘í•œë‹¤.
    https://news.naver.com/main/ranking/popularDay.naver?mid=etc&sid1=111
    """
    url = "https://news.naver.com/main/ranking/popularDay.naver?mid=etc&sid1=111"
    return _extract_naver_news_titles_common(url, limit=limit)


def get_naver_news_economy_titles(limit: int = 50):
    """
    ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœì‹  ê¸°ì‚¬ ì œëª©ì„ ìˆ˜ì§‘í•œë‹¤.
    https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101
    """
    url = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101"
    return _extract_naver_news_titles_common(url, limit=limit)









def collect_all_topics():
    topic_list = []

    # 1. ZUM AI ì´ìŠˆ íŠ¸ë Œë“œ
    zum_topics = get_zum_ai_issue_trends()
    topic_list.extend(zum_topics)
    print(f"âœ… ZUM í‚¤ì›Œë“œ {len(zum_topics)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # 2. Google íŠ¸ë Œë“œ
    google_topics = get_google_trending_keywords()
    topic_list.extend(google_topics)
    print(f"âœ… Google í‚¤ì›Œë“œ {len(google_topics)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # 3. YouTube íŠ¸ë Œë“œ ì˜ìƒ ì œëª©
    youtube_topics = get_youtube_trending_titles()
    topic_list.extend(youtube_topics)
    print(f"âœ… YouTube ì œëª© {len(youtube_topics)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # 4. ì—°í•©ë‰´ìŠ¤ ê±´ê°• ê¸°ì‚¬ ì œëª©

    health_topics = fetch_health_titles()
    topic_list.extend(health_topics)
    print(f"âœ… ì—°í•©ë‰´ìŠ¤ ê±´ê°•ê¸°ì‚¬ {len(health_topics)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    return topic_list


def filter_topics_by_category(topic_list):


    def is_korean(text: str) -> bool:
        return bool(re.search(r"[ê°€-í£]", text))

    # í•œê¸€ë§Œ, ìµœëŒ€ 60ê°œ
    topic_list = [t for t in topic_list if is_korean(t)][:60]

    system_part = (
        "ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ ì½˜í…ì¸  ê¸°íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ì—¬ëŸ¬ ê°œì˜ íŠ¸ë Œë“œ/ë‰´ìŠ¤/ì˜ìƒ ì œëª© ì¤‘ì—ì„œ "
        "í˜„ì¬ ë¸”ë¡œê·¸ì˜ ì¹´í…Œê³ ë¦¬ì™€ ì‹¤ì œë¡œ ê´€ë ¨ì„±ì´ ë†’ê³  ì‚¬ëŒë“¤ì´ ì‹¤ì œë¡œ ê²€ìƒ‰í•´ì„œ ë³¼ ë§Œí•œ ì£¼ì œë§Œ ê³¨ë¼ì•¼ í•©ë‹ˆë‹¤. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„¤ëª…, ë§ë¨¸ë¦¬, ì½”ë“œë¸”ë¡ í‘œì‹œëŠ” ëª¨ë‘ ê¸ˆì§€í•©ë‹ˆë‹¤."
    )

    user_prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ì— ìˆ˜ì§‘í•œ ì£¼ì œ ì œëª© ëª©ë¡ì…ë‹ˆë‹¤:

{topic_list}

ì´ ì¤‘ì—ì„œ ì•„ë˜ ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì£¼ì œ 10ê°œë§Œ ê³¨ë¼ì„œ
JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬]
- ì¹´í…Œê³ ë¦¬: {v_.my_category}
- ìƒì„¸ ë¶„ì•¼/í† í”½: {getattr(v_, 'my_topic', '')}

[ì„ íƒ ê·œì¹™]
1. ìƒí™œ ì •ë³´, ì •ì±…/ì§€ì›ê¸ˆ, ì ˆì•½/ë¹„ìš©ì ˆê°, ê¸ˆìœµ, ì‹¤ìš© íŒ ìª½ì„ ê°€ì¥ ìš°ì„ í•´ì„œ ê³ ë¥¸ë‹¤.
2. ì—°ì˜ˆ, ë‹¨ìˆœ ë¸Œì´ë¡œê·¸, ë¶ˆë¶„ëª…í•œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸, ìœ íŠœë¸Œìš© ìê·¹ ì œëª©ì€ ì œì™¸í•œë‹¤.
3. ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì, ê³¼ë„í•œ ë”°ì˜´í‘œëŠ” ì œê±°ëœ í˜•íƒœë¡œ ë‘”ë‹¤.
4. í•œêµ­ì–´ ì œëª©ë§Œ ì„ íƒí•œë‹¤.
5. ìµœì¢… ì¶œë ¥ì€ ì˜ˆì‹œì²˜ëŸ¼ í•œë‹¤:
["ì „ê¸°ìš”ê¸ˆ ì ˆì•½í•˜ëŠ” 5ê°€ì§€ ë°©ë²•", "2025ë…„ ì •ë¶€ì§€ì›ê¸ˆ ì‹ ì²­ ì´ì •ë¦¬", ...]
6. ë°˜ë“œì‹œ 10ê°œë¥¼ ì¶œë ¥í•œë‹¤.
"""

    # Gemini í˜¸ì¶œ
    try:
        resp_text = call_gemini(
            # ì‹œìŠ¤í…œ ì—­í•  + ìœ ì € ë‚´ìš©ì„ í•©ì³ì„œ í•˜ë‚˜ë¡œ ë³´ë‚¼ê²Œ
            system_part + "\n\n" + user_prompt,
            temperature=0.3,
            is_json=True,  # ê°€ëŠ¥í•˜ë©´ JSONìœ¼ë¡œ ë‹¬ë¼
        )
    except Exception as e:
        print("âŒ í•„í„°ë§ ì‹¤íŒ¨(í˜¸ì¶œ ì˜¤ë¥˜):", e)
        return []

    # ì•ˆì „ì¥ì¹˜
    if not resp_text or resp_text in ("API_ERROR", "SAFETY_BLOCKED"):
        print("âŒ í•„í„°ë§ ì‹¤íŒ¨(Gemini ì‘ë‹µ ì—†ìŒ):", resp_text)
        return []

    # ```json ... ``` ë²—ê²¨ë‚´ê¸°
    cleaned = (
        resp_text.replace("```json", "")
        .replace("```", "")
        .strip()
    )
    print("ğŸ” Gemini ì‘ë‹µ ì›ë¬¸:", cleaned)

    try:
        data = json.loads(cleaned)
        if isinstance(data, list):
            # í˜¹ì‹œ 10ê°œë³´ë‹¤ ë§ì´ ì¤¬ìœ¼ë©´ 10ê°œë§Œ
            return data[:10]
        else:
            print("âš ï¸ JSONì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜:", data)
            return []
    except Exception as e:
        print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return []




def search_naver_blog_top_post(keyword):

    options = Options()
    # options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        query = keyword.replace(" ", "+")
        url = f"https://search.naver.com/search.naver?where=view&query={query}&sm=tab_opt"
        driver.get(url)

        # ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "a.api_txt_lines"))
        )

        elements = driver.find_elements(By.CSS_SELECTOR, "a.api_txt_lines")
        for el in elements:
            href = el.get_attribute("href")
            if "blog.naver.com" in href:
                print("âœ… ìµœìƒë‹¨ ë¸”ë¡œê·¸ ë§í¬:", href)
                return href

        print("âŒ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    except Exception as e:
        print("âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜:", str(e))
        print("ğŸ§ª í˜„ì¬ URL:", driver.current_url)
        print("ğŸ“„ í˜„ì¬ í˜ì´ì§€ ê¸¸ì´:", len(driver.page_source))
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        return None

    finally:
        driver.quit()




# ì˜ˆì‹œ
# search_naver_blog_top_post("ì „ê¸°ìš”ê¸ˆ í• ì¸ ì œë„")



#$ êµ¬ê¸€ íŠ¸ë Œë“œ



def search_web_for_keyword(keyword: str, num_results: int = 3) -> list[dict] | bool:
    """
    ì£¼ì–´ì§„ í‚¤ì›Œë“œë¡œ êµ¬ê¸€ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê° URLì— ì ‘ì†í•˜ì—¬
    ì œëª©(title)ê³¼ ìš”ì•½(snippet)ì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ.
        num_results (int): ì²˜ë¦¬í•  ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜. ê¸°ë³¸ê°’ì€ 3ê°œ.

    Returns:
        list[dict] | bool:
            - ì„±ê³µ ì‹œ: ê° ê²°ê³¼ê°€ title, link, snippetì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.
            - ì‹¤íŒ¨ ì‹œ: Falseë¥¼ ë°˜í™˜.
    """
    if not keyword:
        print("âŒ ì—ëŸ¬: ê²€ìƒ‰ì–´(keyword)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return False

    print(f"â–¶ ì›¹ ê²€ìƒ‰ ì‹¤í–‰: '{keyword}' (ê²°ê³¼ {num_results}ê°œ ìš”ì²­)")

    try:
        # 1. êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•´ URL ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (Generatorë¥¼ listë¡œ ë³€í™˜)
        # lang="ko"ë¡œ í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        urls = list(search(keyword, num_results=num_results, lang="ko"))

        if not urls:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: {len(urls)}ê°œ. ì´ì œ ê° í˜ì´ì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")

        formatted_results = []
        # 2. ê° URLì— ì ‘ì†í•˜ì—¬ ì œëª©ê³¼ ë‚´ìš©(snippet) ì¶”ì¶œ
        for url in urls:
            try:
                # ì›¹ì‚¬ì´íŠ¸ê°€ ì°¨ë‹¨í•˜ì§€ ì•Šë„ë¡ User-Agentë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                response = requests.get(url, headers=headers, timeout=5)
                response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

                soup = BeautifulSoup(response.text, 'html.parser')

                # ì œëª© ì¶”ì¶œ (og:title, ì—†ìœ¼ë©´ <title> íƒœê·¸)
                title = soup.find("meta", property="og:title")
                if title:
                    title = title.get("content")
                else:
                    title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"

                # ìš”ì•½ ì¶”ì¶œ (og:description, ì—†ìœ¼ë©´ meta description)
                snippet = soup.find("meta", property="og:description")
                if snippet:
                    snippet = snippet.get("content")
                else:
                    snippet = soup.find("meta", attrs={"name": "description"})
                    if snippet:
                        snippet = snippet.get("content")
                    else:
                        # ìœ„ íƒœê·¸ë“¤ì´ ì—†ì„ ê²½ìš°, ì²« ë²ˆì§¸ <p> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¼ë¶€ ì‚¬ìš©
                        first_p = soup.find('p')
                        snippet = first_p.get_text() if first_p else "ë‚´ìš© ìš”ì•½ ì—†ìŒ"

                formatted_results.append({
                    "title": title.strip(),
                    "link": url,
                    "snippet": snippet.strip()
                })
                print(f"  - ì„±ê³µ: {url}")

            except requests.exceptions.RequestException as e:
                # íŠ¹ì • í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê³  ë‹¤ìŒ URL ì²˜ë¦¬
                print(f"  - ì‹¤íŒ¨: {url} (ì´ìœ : {e})")
                continue  # ë‹¤ìŒ ë£¨í”„ë¡œ ë„˜ì–´ê°
            except Exception as e:
                print(f"  - ì‹¤íŒ¨: {url} (ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬: {e})")
                continue

        if not formatted_results:
            print(f"âŒ ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: ìˆ˜ì§‘í•œ URLì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return False

        print(f"âœ… ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: ìµœì¢…ì ìœ¼ë¡œ ìœ íš¨í•œ ê²°ê³¼ {len(formatted_results)}ê°œ ìˆ˜ì§‘")
        return formatted_results

    except Exception:
        error_details = traceback.format_exc()
        print(f"âŒ ì¹˜ëª…ì  ê²€ìƒ‰ ì—ëŸ¬ ë°œìƒ: {error_details}")
        return False

#
# if __name__ == '__main__':
#     # ì´ íŒŒì¼(trend_search_page.py)ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ ë™ì‘í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì½”ë“œ
#     print("--- search_web_for_keyword í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ---")
#
#     test_keyword = "íŒŒì´ì¬ ë¸”ë¡œê·¸ ìë™í™”"
#     search_results = search_web_for_keyword(test_keyword, num_results=3)
#
#     if search_results:
#         print(f"\n[í…ŒìŠ¤íŠ¸ ê²°ê³¼: '{test_keyword}']")
#         for i, result in enumerate(search_results, 1):
#             print(f"  {i}. ì œëª©: {result['title']}")
#             print(f"     ë§í¬: {result['link']}")
#             print(f"     ë‚´ìš©: {result['snippet'][:80]}...")
#             print("-" * 20)
#     else:
#         print(f"\n[í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨] '{test_keyword}'ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ë‰´ìŠ¤ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬(ì‚¬íšŒ/ìƒí™œ/IT) ì´ìŠˆ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_naver_news_multi_category_topics(
    max_items_per_category: int = 30,
) -> list[dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ 'ì‚¬íšŒ/ìƒí™œÂ·ë¬¸í™”/ITÂ·ê³¼í•™' ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ê¸°ì‚¬ ì œëª©ì„ ì¶”ì¶œí•œë‹¤.

    ë°˜í™˜ í˜•ì‹:
    [
        {"source": "naver_news_society", "title": "...", "rank": 1, "url": "https://..."},
        {"source": "naver_news_life", "title": "...", "rank": 1, "url": "https://..."},
        ...
    ]
    """


    # ë„¤ì´ë²„ ì¹´í…Œê³ ë¦¬: sid1 ê°’
    category_map = {
        "naver_news_society": "102",  # ì‚¬íšŒ
        "naver_news_life": "103",     # ìƒí™œ/ë¬¸í™”
        "naver_news_it": "105",       # IT/ê³¼í•™
    }

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        )
    }

    base_url = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid}"

    all_topics: list[dict] = []
    session = requests.Session()

    for source, sid in category_map.items():
        url = base_url.format(sid=sid)
        try:
            resp = session.get(url, headers=headers, timeout=10)
            resp.encoding = "utf-8"
        except Exception as e:
            print(f"âš  ë„¤ì´ë²„ ì¹´í…Œê³ ë¦¬ ìš”ì²­ ì‹¤íŒ¨ [{source}]: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        # ë„¤ì´ë²„ ê¸°ì‚¬ ë§í¬ íŒ¨í„´: /read?oid=...&aid=...
        raw_items: list[tuple[str, str]] = []
        for a in soup.select("a[href*='/read?']"):
            title = (a.get_text() or "").strip()
            if not title:
                continue
            if len(title) < 8 or len(title) > 80:
                continue

            href = a.get("href") or ""
            if href.startswith("/"):
                href = "https://news.naver.com" + href

            raw_items.append((title, href))

        # ì¤‘ë³µ ì œê±° + ìƒí•œ ì¡°ì ˆ
        seen_titles: set[str] = set()
        rank = 1
        for title, link in raw_items:
            if title in seen_titles:
                continue
            seen_titles.add(title)

            all_topics.append(
                {
                    "source": source,
                    "title": title,
                    "rank": rank,
                    "url": link,
                }
            )
            rank += 1
            if rank > max_items_per_category:
                break

        print(
            f"âœ… [NAVER MULTI] {source} ({sid}) â†’ {rank - 1}ê°œ ìˆ˜ì§‘"
        )

    return all_topics


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—°í•©ë‰´ìŠ¤ íŠ¸ë Œë“œ/ì¸ê¸°/ìƒí™œ/ê²½ì œ/IT ë‰´ìŠ¤ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_yonhap_news_trend_topics(
    max_items_per_category: int = 30,
) -> list[dict]:
    """
    ì—°í•©ë‰´ìŠ¤ì—ì„œ ì¸ê¸°/í•«ë‰´ìŠ¤/ìƒí™œ/ê²½ì œ/ì‚°ì—…(ITÂ·ê³¼í•™ í¬í•¨) ê¸°ì‚¬ ì œëª©ì„ ì¶”ì¶œí•œë‹¤.

    ë°˜í™˜ í˜•ì‹:
    [
        {"source": "yonhap_popular", "title": "...", "rank": 1, "url": "https://..."},
        {"source": "yonhap_hotnews", "title": "...", "rank": 1, "url": "https://..."},
        ...
    ]
    """


    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        )
    }

    # ì—°í•©ë‰´ìŠ¤ ì£¼ìš” íŠ¸ë Œë“œ/ë¼ì´í”„ ì¹´í…Œê³ ë¦¬
    url_map = {
        "yonhap_popular": "https://www.yna.co.kr/theme/popular?site=navi_popular",
        "yonhap_hotnews": "https://www.yna.co.kr/theme/hotnews",
        "yonhap_life": "https://www.yna.co.kr/lifestyle/all",
        "yonhap_economy": "https://www.yna.co.kr/economy/all",
        "yonhap_it_science": "https://www.yna.co.kr/industry/common",
    }

    all_topics: list[dict] = []
    session = requests.Session()

    for source, url in url_map.items():
        try:
            resp = session.get(url, headers=headers, timeout=10)
            resp.encoding = "utf-8"
        except Exception as e:
            print(f"âš  ì—°í•©ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨ [{source}]: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        # ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ íŒ¨í„´: /view/ì—°ë„...
        raw_items: list[tuple[str, str]] = []
        for a in soup.select("a[href*='/view/']"):
            title = (a.get_text() or "").strip()
            if not title:
                continue
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì œëª©ì€ ì œì™¸
            if len(title) < 8 or len(title) > 80:
                continue

            href = a.get("href") or ""
            if href.startswith("/"):
                href = "https://www.yna.co.kr" + href

            raw_items.append((title, href))

        seen_titles: set[str] = set()
        rank = 1
        for title, link in raw_items:
            if title in seen_titles:
                continue
            seen_titles.add(title)

            all_topics.append(
                {
                    "source": source,
                    "title": title,
                    "rank": rank,
                    "url": link,
                }
            )
            rank += 1
            if rank > max_items_per_category:
                break

        print(f"âœ… [YONHAP] {source} â†’ {rank - 1}ê°œ ìˆ˜ì§‘")

    return all_topics





